{"cells":[{"cell_type":"markdown","source":["# Analyzing Web Server Logs with Apache Spark\n\nApache Spark is an excellent and ideal framework for wrangling, analyzing and modeling on structured and unstructured data - at scale! In this mini-project, we will be focusing on one of the most popular use-cases in the industry - log analytics.\n\nTypically, server logs are a very common data source in enterprises and often contain a gold mine of actionable insights and information. Log data comes from many sources in an enterprise, such as the web, client and compute servers, applications, user-generated content, flat files. They can be used for monitoring servers, improving business and customer intelligence, building recommendation systems, fraud detection, and much more.\n\nSpark allows you to dump and store your logs in files on disk cheaply, while still providing rich APIs to perform data analysis at scale. This mini-project will show you how to use Apache Spark on real-world production logs from NASA.\nYou will complete the extract, transform, and load (ETL) process in this Apache Spark enviroment. During this process, you will learn why the ETL process is so crucial to the quality of the machine learning work we will be doing later on.\n\n\nThere is a total of 15 questions for you to solve along with some interactive examples which will help you learn aspects of leveraging spark for analyzing over 3 million logs at scale.\n\nRemember to focus on the __`# TODO: Replace <FILL IN> with appropriate code`__ sections to fill them up with necessary code to solve the desired questions in the notebook"],"metadata":{"colab_type":"text","id":"q0ap-hmpXu-s","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dceed682-ffa7-41ad-955e-e5ab0e0b421e"}}},{"cell_type":"markdown","source":["# Data extraction:"],"metadata":{"colab_type":"text","id":"6IkR5OAXYBVV","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aed3386a-6a61-442e-bcfe-d757c562a124"}}},{"cell_type":"markdown","source":["# Step 1 - Loading up Dependencies"],"metadata":{"colab_type":"text","id":"W4XXOPlzXu-7","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c74d074-6cc3-44fe-8ada-40ade6a47b14"}}},{"cell_type":"code","source":["spark"],"metadata":{"colab_type":"code","id":"zYGRL15uXu--","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8136f21-adb2-4a94-b768-ebaffd50b2f9"},"colab":{},"outputId":"34c065c7-f451-4543-dde3-5ddbbcada8c6"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.242.115:46389\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":"<div class=\"ansiout\">Out[1]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.242.115:46389\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["sqlContext"],"metadata":{"colab_type":"code","id":"TQ9VOjOGXu_N","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efb3ead4-9f84-402f-b78f-7a894af984e1"},"colab":{},"outputId":"8c2ee3dc-5d83-475e-c77a-3b83e98cff42"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: &lt;pyspark.sql.context.HiveContext at 0x7f490491fbd0&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &lt;pyspark.sql.context.HiveContext at 0x7f490491fbd0&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["if 'sc' not in locals():\n    from pyspark.context import SparkContext\n    from pyspark.sql.context import SQLContext\n    from pyspark.sql.session import SparkSession\n    \n    sc = SparkContext()\n    sqlContext = SQLContext(sc)\n    spark = SparkSession(sc)"],"metadata":{"colab_type":"code","id":"IM6ajIMVXu_V","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eeb83de6-0123-4df8-9e9b-146b1555fdeb"},"colab":{},"outputId":"d49062e6-607c-42bf-e2de-5019dee77ce0"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import re\nimport pandas as pd"],"metadata":{"colab_type":"code","id":"r3E2N5WCXu_b","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"629fc02a-81d2-4e7b-be4a-fab33588b713"},"colab":{},"outputId":"cb9f5a8a-06b5-407a-c634-a703b3e5b92b"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["m = re.finditer(r'.*?(spark).*?', \"I'm searching for a spark in PySpark\", re.I)\nfor match in m:\n    print(match)"],"metadata":{"colab_type":"code","id":"UupCPIruXu_i","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52a7db21-2cab-467d-8571-a30dd8183d50"},"colab":{},"outputId":"ae83b30e-9b7e-458f-fd16-94d9d738e589"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;re.Match object; span=(0, 25), match=&#34;I&#39;m searching for a spark&#34;&gt;\n&lt;re.Match object; span=(25, 36), match=&#39; in PySpark&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;re.Match object; span=(0, 25), match=&#34;I&#39;m searching for a spark&#34;&gt;\n&lt;re.Match object; span=(25, 36), match=&#39; in PySpark&#39;&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["For this mini-project, we will analyze datasets from NASA Kennedy Space Center web server in Florida. The full data set is freely available for download [__here__](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).\n\nThese two traces contain two month's worth of all HTTP requests to the NASA Kennedy Space Center WWW server in Florida. You can head over to the [__website__](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) and download the following files if needed OR just upload the files we have already provided for you into Domino's Cloud Platform (unless you plan to use Spark locally).\n\n- Jul 01 to Jul 31, ASCII format, 20.7 MB gzip compressed, 205.2 MB uncompressed: [ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz)\n- Aug 04 to Aug 31, ASCII format, 21.8 MB gzip compressed, 167.8 MB uncompressed: [ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz)\n\nMake sure both the data files have been uploaded to Databricks under **\"Data\" > \"DBFS\" > \"Tables\"** as a **.txt** file\n\n\n![DBFS](https://drive.google.com/uc?id=1eE9_CgnUW7psBs_Nlk9qrdD2dXh1sU9A)"],"metadata":{"colab_type":"text","id":"Pln9za4iXu_n","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"807ebeb2-0891-426b-8d50-c81bff19a19a"}}},{"cell_type":"markdown","source":["# Step 2 - Loading and Viewing the Log Dataset\n\nGiven that our data is stored in the following mentioned path, let's load it into a DataFrame. We'll do this in steps. First, we'll use `sqlContext.read.text()` or `spark.read.text()` to read the text file. This will produce a DataFrame with a single string column called `value`."],"metadata":{"colab_type":"text","id":"KESqVXmAXu_p","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85efa34c-35b4-4ced-ae2a-eae63d1abb87"}}},{"cell_type":"markdown","source":["### Taking a look at the metadata of our dataframe"],"metadata":{"colab_type":"text","id":"pis2_lWcXu_u","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6767f56-2cbe-451a-a909-f3a7f59452d8"}}},{"cell_type":"code","source":["# make sure you have upload NASA_access_log_Aug95.txt and NASA_access_log_Jul95.txt onto Spark before you run the following code\n\nbase_df = spark.read.text('dbfs:/FileStore/tables/*.txt')\nbase_df.printSchema()"],"metadata":{"colab_type":"code","id":"f4-znXsmXu_w","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3c06e9f-45e0-4ad3-bcdb-00323e55419f"},"colab":{},"outputId":"5c326d84-2124-47c4-c63f-e70665bfb820"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"base_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"value","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- value: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- value: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["type(base_df)"],"metadata":{"colab_type":"code","id":"IBHQ8QQkXu_0","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe1e5415-ec7e-4736-9b85-2c90f159e13e"},"colab":{},"outputId":"14c57eda-586c-4a28-9e1c-df9364bf0ff8"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: pyspark.sql.dataframe.DataFrame</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: pyspark.sql.dataframe.DataFrame</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can also convert a dataframe to an RDD if needed"],"metadata":{"colab_type":"text","id":"gOKkUBQvXu_5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08e625b1-03b2-49e2-93df-c30f76c8027f"}}},{"cell_type":"code","source":["base_df_rdd = base_df.rdd\ntype(base_df_rdd)"],"metadata":{"colab_type":"code","id":"hMByB80sXu_6","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0de09a08-a0ee-467b-ac38-5227d94980cd"},"colab":{},"outputId":"29f8e5c6-aecf-4f81-d1e8-40a5ef92eb18"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: pyspark.rdd.RDD</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: pyspark.rdd.RDD</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Viewing sample data in our dataframe\nLooks like it needs to be wrangled and parsed!"],"metadata":{"colab_type":"text","id":"oi1QtXBhXvAB","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9f336ad-c700-4789-aaba-8edc03a77e93"}}},{"cell_type":"code","source":["base_df.show(10, truncate=False)"],"metadata":{"colab_type":"code","id":"iertYldRXvAD","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91a18e23-357f-4e62-a6c0-29a1bd957efb"},"colab":{},"outputId":"cf7a1843-1287-455e-a32c-e67a498a89c0"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                     |\n+--------------------------------------------------------------------------------------------------------------------------+\n|in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] &#34;GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0&#34; 200 1839|\n|uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] &#34;GET / HTTP/1.0&#34; 304 0                                                   |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/ksclogo-medium.gif HTTP/1.0&#34; 304 0                          |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/MOSAIC-logosmall.gif HTTP/1.0&#34; 304 0                        |\n|uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/USA-logosmall.gif HTTP/1.0&#34; 304 0                           |\n|ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] &#34;GET /images/launch-logo.gif HTTP/1.0&#34; 200 1713              |\n|uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/WORLD-logosmall.gif HTTP/1.0&#34; 304 0                         |\n|slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] &#34;GET /history/skylab/skylab.html HTTP/1.0&#34; 200 1687                 |\n|piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/launchmedium.gif HTTP/1.0&#34; 200 11853                   |\n|slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] &#34;GET /history/skylab/skylab-small.gif HTTP/1.0&#34; 200 9202            |\n+--------------------------------------------------------------------------------------------------------------------------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------+\nvalue                                                                                                                     |\n+--------------------------------------------------------------------------------------------------------------------------+\nin24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] &#34;GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0&#34; 200 1839|\nuplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] &#34;GET / HTTP/1.0&#34; 304 0                                                   |\nuplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/ksclogo-medium.gif HTTP/1.0&#34; 304 0                          |\nuplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/MOSAIC-logosmall.gif HTTP/1.0&#34; 304 0                        |\nuplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/USA-logosmall.gif HTTP/1.0&#34; 304 0                           |\nix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] &#34;GET /images/launch-logo.gif HTTP/1.0&#34; 200 1713              |\nuplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/WORLD-logosmall.gif HTTP/1.0&#34; 304 0                         |\nslppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] &#34;GET /history/skylab/skylab.html HTTP/1.0&#34; 200 1687                 |\npiweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/launchmedium.gif HTTP/1.0&#34; 200 11853                   |\nslppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] &#34;GET /history/skylab/skylab-small.gif HTTP/1.0&#34; 200 9202            |\n+--------------------------------------------------------------------------------------------------------------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Getting data from an RDD is slightly different. You can see how the data representation is different in the following RDD"],"metadata":{"colab_type":"text","id":"T9aeiekzXvAJ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1e4bcdc-e917-4f6e-92f7-35e952abc480"}}},{"cell_type":"code","source":["base_df_rdd.take(10)"],"metadata":{"colab_type":"code","id":"PCdKKlCeXvAK","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e42a6641-d38f-48c7-b19b-c7c9ea1219a0"},"colab":{},"outputId":"58876be0-2960-4df1-cefa-75daa1462dbd"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[10]: [Row(value=&#39;in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] &#34;GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0&#34; 200 1839&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] &#34;GET / HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/ksclogo-medium.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/MOSAIC-logosmall.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/USA-logosmall.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] &#34;GET /images/launch-logo.gif HTTP/1.0&#34; 200 1713&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/WORLD-logosmall.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] &#34;GET /history/skylab/skylab.html HTTP/1.0&#34; 200 1687&#39;),\n Row(value=&#39;piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/launchmedium.gif HTTP/1.0&#34; 200 11853&#39;),\n Row(value=&#39;slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] &#34;GET /history/skylab/skylab-small.gif HTTP/1.0&#34; 200 9202&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [Row(value=&#39;in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] &#34;GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0&#34; 200 1839&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] &#34;GET / HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/ksclogo-medium.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/MOSAIC-logosmall.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/USA-logosmall.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] &#34;GET /images/launch-logo.gif HTTP/1.0&#34; 200 1713&#39;),\n Row(value=&#39;uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/WORLD-logosmall.gif HTTP/1.0&#34; 304 0&#39;),\n Row(value=&#39;slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] &#34;GET /history/skylab/skylab.html HTTP/1.0&#34; 200 1687&#39;),\n Row(value=&#39;piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/launchmedium.gif HTTP/1.0&#34; 200 11853&#39;),\n Row(value=&#39;slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] &#34;GET /history/skylab/skylab-small.gif HTTP/1.0&#34; 200 9202&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Data transformation"],"metadata":{"colab_type":"text","id":"asaanbGIYfR8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cab4f2bc-680c-4a93-8fdb-5d75d9be2882"}}},{"cell_type":"markdown","source":["# Step 1 - Data Wrangling\n\nIn this section, we will try and clean and parse our log dataset to really extract structured attributes with meaningful information from each log message.\n\n### Data understanding\nIf you're familiar with web server logs, you'll recognize that the above displayed data is in [Common Log Format](https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format). \n\nThe fields are:\n__`remotehost rfc931 authuser [date] \"request\" status bytes`__\n\n\n| field         | meaning                                                                |\n| ------------- | ---------------------------------------------------------------------- |\n| _remotehost_  | Remote hostname (or IP number if DNS hostname is not available or if [DNSLookup](https://www.w3.org/Daemon/User/Config/General.html#DNSLookup) is off).       |\n| _rfc931_      | The remote logname of the user if at all it is present. |\n| _authuser_    | The username of the remote user after authentication by the HTTP server.  |\n| _[date]_      | Date and time of the request.                                      |\n| _\"request\"_   | The request, exactly as it came from the browser or client.            |\n| _status_      | The [HTTP status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) the server sent back to the client.               |\n| _bytes_       | The number of bytes (`Content-Length`) transferred to the client.      |\n\nWe will need to use some specific techniques to parse, match and extract these attributes from the log data"],"metadata":{"colab_type":"text","id":"yv5kCt3xXvAP","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99b05871-f347-4f44-aea1-5e6991875d88"}}},{"cell_type":"markdown","source":["## Data Parsing and Extraction with Regular Expressions\n\nNext, we have to parse it into individual columns. We'll use the special built-in [regexp\\_extract()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract)\nfunction to do the parsing. This function matches a column against a regular expression with one or more [capture groups](http://regexone.com/lesson/capturing_groups) and allows you to extract one of the matched groups. We'll use one regular expression for each field we wish to extract.\n\nYou must have heard or used a fair bit of regular expressions by now. If you find regular expressions confusing (and they certainly _can_ be), and you want to learn more about them, we recommend checking out the\n[RegexOne web site](http://regexone.com/). You might also find [_Regular Expressions Cookbook_](http://shop.oreilly.com/product/0636920023630.do), by Goyvaerts and Levithan, to be useful as a reference."],"metadata":{"colab_type":"text","id":"9aCEcwBLXvAQ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f57fda5f-f458-45b4-a5ed-ec52467baea5"}}},{"cell_type":"markdown","source":["#### Let's take a look at our dataset dimensions"],"metadata":{"colab_type":"text","id":"a3TFNjb1XvAS","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59df1d8f-2f0f-434f-871a-b304ac5597a6"}}},{"cell_type":"code","source":["print((base_df.count(), len(base_df.columns)))"],"metadata":{"colab_type":"code","id":"ncbKGPMJXvAT","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba088b72-0cea-4cf0-845e-fd3bf0baf7dd"},"colab":{},"outputId":"2d0a17f7-5bd7-444a-9a65-f547bf601f09"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">(3461613, 1)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(3461613, 1)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's extract and take a look at some sample log messages"],"metadata":{"colab_type":"text","id":"dtZOfHsVXvAZ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e037781-93a3-4fdd-a571-cb4ab184d53b"}}},{"cell_type":"code","source":["sample_logs = [item['value'] for item in base_df.take(15)]\nsample_logs"],"metadata":{"colab_type":"code","id":"-3sfUy4NXvAa","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8a7077b-a0cb-4935-8e42-effe04863397"},"colab":{},"outputId":"056e2311-d4be-4c1b-e507-a4c6f1868ace"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[12]: [&#39;in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] &#34;GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0&#34; 200 1839&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] &#34;GET / HTTP/1.0&#34; 304 0&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/ksclogo-medium.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/MOSAIC-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/USA-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] &#34;GET /images/launch-logo.gif HTTP/1.0&#34; 200 1713&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/WORLD-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] &#34;GET /history/skylab/skylab.html HTTP/1.0&#34; 200 1687&#39;,\n &#39;piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/launchmedium.gif HTTP/1.0&#34; 200 11853&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] &#34;GET /history/skylab/skylab-small.gif HTTP/1.0&#34; 200 9202&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:12 -0400] &#34;GET /images/ksclogosmall.gif HTTP/1.0&#34; 200 3635&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:12 -0400] &#34;GET /history/apollo/images/apollo-logo1.gif HTTP/1.0&#34; 200 1173&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:13 -0400] &#34;GET /history/apollo/images/apollo-logo.gif HTTP/1.0&#34; 200 3047&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:14 -0400] &#34;GET /images/NASA-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;133.43.96.45 - - [01/Aug/1995:00:00:16 -0400] &#34;GET /shuttle/missions/sts-69/mission-sts-69.html HTTP/1.0&#34; 200 10566&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [&#39;in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] &#34;GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0&#34; 200 1839&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:07 -0400] &#34;GET / HTTP/1.0&#34; 304 0&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/ksclogo-medium.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/MOSAIC-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] &#34;GET /images/USA-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:09 -0400] &#34;GET /images/launch-logo.gif HTTP/1.0&#34; 200 1713&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/WORLD-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:10 -0400] &#34;GET /history/skylab/skylab.html HTTP/1.0&#34; 200 1687&#39;,\n &#39;piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] &#34;GET /images/launchmedium.gif HTTP/1.0&#34; 200 11853&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:11 -0400] &#34;GET /history/skylab/skylab-small.gif HTTP/1.0&#34; 200 9202&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:12 -0400] &#34;GET /images/ksclogosmall.gif HTTP/1.0&#34; 200 3635&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com - - [01/Aug/1995:00:00:12 -0400] &#34;GET /history/apollo/images/apollo-logo1.gif HTTP/1.0&#34; 200 1173&#39;,\n &#39;slppp6.intermind.net - - [01/Aug/1995:00:00:13 -0400] &#34;GET /history/apollo/images/apollo-logo.gif HTTP/1.0&#34; 200 3047&#39;,\n &#39;uplherc.upl.com - - [01/Aug/1995:00:00:14 -0400] &#34;GET /images/NASA-logosmall.gif HTTP/1.0&#34; 304 0&#39;,\n &#39;133.43.96.45 - - [01/Aug/1995:00:00:16 -0400] &#34;GET /shuttle/missions/sts-69/mission-sts-69.html HTTP/1.0&#34; 200 10566&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Extracting host names\n\nLet's try and write some regular expressions to extract the host name from the logs"],"metadata":{"colab_type":"text","id":"0aPYBjkwXvAf","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"634b80f6-5c66-4377-bb4d-0bc2b540a1be"}}},{"cell_type":"code","source":["host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\nhosts = [re.search(host_pattern, item).group(1)\n           if re.search(host_pattern, item)\n           else 'no match'\n           for item in sample_logs]\nhosts"],"metadata":{"colab_type":"code","id":"WarNo8bGXvAh","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82d7a45b-2260-409c-89e0-744de52233c4"},"colab":{},"outputId":"d8a548f2-335e-4ab4-8a9f-28f55060e540"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[13]: [&#39;in24.inetnebr.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;piweba4y.prodigy.com&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;133.43.96.45&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [&#39;in24.inetnebr.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;piweba4y.prodigy.com&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;ix-esc-ca2-07.ix.netcom.com&#39;,\n &#39;slppp6.intermind.net&#39;,\n &#39;uplherc.upl.com&#39;,\n &#39;133.43.96.45&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Extracting timestamps \n\nLet's now try and use regular expressions to extract the timestamp fields from the logs"],"metadata":{"colab_type":"text","id":"TcZrGpgwXvAk","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03e464da-8852-4cb0-b31f-6f0cb466c7ed"}}},{"cell_type":"code","source":["ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\ntimestamps = [re.search(ts_pattern, item).group(1) for item in sample_logs]\ntimestamps"],"metadata":{"colab_type":"code","id":"GOot8niKXvAl","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49126a1c-8225-4b9d-a4c1-166b582f5f91"},"colab":{},"outputId":"6ad3338e-8ff0-48f1-c4d0-ad52742232b9"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: [&#39;01/Aug/1995:00:00:01 -0400&#39;,\n &#39;01/Aug/1995:00:00:07 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:09 -0400&#39;,\n &#39;01/Aug/1995:00:00:10 -0400&#39;,\n &#39;01/Aug/1995:00:00:10 -0400&#39;,\n &#39;01/Aug/1995:00:00:10 -0400&#39;,\n &#39;01/Aug/1995:00:00:11 -0400&#39;,\n &#39;01/Aug/1995:00:00:12 -0400&#39;,\n &#39;01/Aug/1995:00:00:12 -0400&#39;,\n &#39;01/Aug/1995:00:00:13 -0400&#39;,\n &#39;01/Aug/1995:00:00:14 -0400&#39;,\n &#39;01/Aug/1995:00:00:16 -0400&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [&#39;01/Aug/1995:00:00:01 -0400&#39;,\n &#39;01/Aug/1995:00:00:07 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:09 -0400&#39;,\n &#39;01/Aug/1995:00:00:10 -0400&#39;,\n &#39;01/Aug/1995:00:00:10 -0400&#39;,\n &#39;01/Aug/1995:00:00:10 -0400&#39;,\n &#39;01/Aug/1995:00:00:11 -0400&#39;,\n &#39;01/Aug/1995:00:00:12 -0400&#39;,\n &#39;01/Aug/1995:00:00:12 -0400&#39;,\n &#39;01/Aug/1995:00:00:13 -0400&#39;,\n &#39;01/Aug/1995:00:00:14 -0400&#39;,\n &#39;01/Aug/1995:00:00:16 -0400&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Extracting HTTP Request Method, URIs and Protocol \n\nLet's now try and use regular expressions to extract the HTTP request methods, URIs and Protocol patterns fields from the logs"],"metadata":{"colab_type":"text","id":"CKJ5F8tCXvAo","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53304337-522d-4a18-a6b8-26bbfacba6c0"}}},{"cell_type":"code","source":["method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\nmethod_uri_protocol = [re.search(method_uri_protocol_pattern, item).groups()\n               if re.search(method_uri_protocol_pattern, item)\n               else 'no match'\n              for item in sample_logs]\nmethod_uri_protocol"],"metadata":{"colab_type":"code","id":"TZvnPVXeXvAp","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0a35287-9ef9-4434-8f2a-447d5b1513d2"},"colab":{},"outputId":"f3cdd71d-0239-43a4-fd8f-245193f571c3"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[15]: [(&#39;GET&#39;, &#39;/shuttle/missions/sts-68/news/sts-68-mcc-05.txt&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/ksclogo-medium.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/MOSAIC-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/USA-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/launch-logo.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/WORLD-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/skylab/skylab.html&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/launchmedium.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/skylab/skylab-small.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/ksclogosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/apollo/images/apollo-logo1.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/apollo/images/apollo-logo.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/NASA-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/shuttle/missions/sts-69/mission-sts-69.html&#39;, &#39;HTTP/1.0&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: [(&#39;GET&#39;, &#39;/shuttle/missions/sts-68/news/sts-68-mcc-05.txt&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/ksclogo-medium.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/MOSAIC-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/USA-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/launch-logo.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/WORLD-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/skylab/skylab.html&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/launchmedium.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/skylab/skylab-small.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/ksclogosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/apollo/images/apollo-logo1.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/history/apollo/images/apollo-logo.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/images/NASA-logosmall.gif&#39;, &#39;HTTP/1.0&#39;),\n (&#39;GET&#39;, &#39;/shuttle/missions/sts-69/mission-sts-69.html&#39;, &#39;HTTP/1.0&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Building an intermediate parsed dataframe\n\nLet's try and use our regular expressions we have implemented so far into parsing and extracting the relevant entities in separate columns in a new dataframe"],"metadata":{"colab_type":"text","id":"oaBizluJXvAu","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6f6b989-a916-46bb-af97-ea5d110ce048"}}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_extract\n\nlogs_df = base_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n                         regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'))\nlogs_df.show(10, truncate=False)\nprint((logs_df.count(), len(logs_df.columns)))"],"metadata":{"colab_type":"code","id":"MLI8LUVPXvAv","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8d48e16-6451-4834-b5f1-19cb97fbf360"},"colab":{},"outputId":"5c8becac-e3bb-4a61-cc8e-bba69c4169d6"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"logs_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+---------------------------+--------------------------+------+-----------------------------------------------+--------+\n|host                       |timestamp                 |method|endpoint                                       |protocol|\n+---------------------------+--------------------------+------+-----------------------------------------------+--------+\n|in24.inetnebr.com          |01/Aug/1995:00:00:01 -0400|GET   |/shuttle/missions/sts-68/news/sts-68-mcc-05.txt|HTTP/1.0|\n|uplherc.upl.com            |01/Aug/1995:00:00:07 -0400|GET   |/                                              |HTTP/1.0|\n|uplherc.upl.com            |01/Aug/1995:00:00:08 -0400|GET   |/images/ksclogo-medium.gif                     |HTTP/1.0|\n|uplherc.upl.com            |01/Aug/1995:00:00:08 -0400|GET   |/images/MOSAIC-logosmall.gif                   |HTTP/1.0|\n|uplherc.upl.com            |01/Aug/1995:00:00:08 -0400|GET   |/images/USA-logosmall.gif                      |HTTP/1.0|\n|ix-esc-ca2-07.ix.netcom.com|01/Aug/1995:00:00:09 -0400|GET   |/images/launch-logo.gif                        |HTTP/1.0|\n|uplherc.upl.com            |01/Aug/1995:00:00:10 -0400|GET   |/images/WORLD-logosmall.gif                    |HTTP/1.0|\n|slppp6.intermind.net       |01/Aug/1995:00:00:10 -0400|GET   |/history/skylab/skylab.html                    |HTTP/1.0|\n|piweba4y.prodigy.com       |01/Aug/1995:00:00:10 -0400|GET   |/images/launchmedium.gif                       |HTTP/1.0|\n|slppp6.intermind.net       |01/Aug/1995:00:00:11 -0400|GET   |/history/skylab/skylab-small.gif               |HTTP/1.0|\n+---------------------------+--------------------------+------+-----------------------------------------------+--------+\nonly showing top 10 rows\n\n(3461613, 5)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------+--------------------------+------+-----------------------------------------------+--------+\nhost                       |timestamp                 |method|endpoint                                       |protocol|\n+---------------------------+--------------------------+------+-----------------------------------------------+--------+\nin24.inetnebr.com          |01/Aug/1995:00:00:01 -0400|GET   |/shuttle/missions/sts-68/news/sts-68-mcc-05.txt|HTTP/1.0|\nuplherc.upl.com            |01/Aug/1995:00:00:07 -0400|GET   |/                                              |HTTP/1.0|\nuplherc.upl.com            |01/Aug/1995:00:00:08 -0400|GET   |/images/ksclogo-medium.gif                     |HTTP/1.0|\nuplherc.upl.com            |01/Aug/1995:00:00:08 -0400|GET   |/images/MOSAIC-logosmall.gif                   |HTTP/1.0|\nuplherc.upl.com            |01/Aug/1995:00:00:08 -0400|GET   |/images/USA-logosmall.gif                      |HTTP/1.0|\nix-esc-ca2-07.ix.netcom.com|01/Aug/1995:00:00:09 -0400|GET   |/images/launch-logo.gif                        |HTTP/1.0|\nuplherc.upl.com            |01/Aug/1995:00:00:10 -0400|GET   |/images/WORLD-logosmall.gif                    |HTTP/1.0|\nslppp6.intermind.net       |01/Aug/1995:00:00:10 -0400|GET   |/history/skylab/skylab.html                    |HTTP/1.0|\npiweba4y.prodigy.com       |01/Aug/1995:00:00:10 -0400|GET   |/images/launchmedium.gif                       |HTTP/1.0|\nslppp6.intermind.net       |01/Aug/1995:00:00:11 -0400|GET   |/history/skylab/skylab-small.gif               |HTTP/1.0|\n+---------------------------+--------------------------+------+-----------------------------------------------+--------+\nonly showing top 10 rows\n\n(3461613, 5)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Extracting HTTP Status Codes\n\nLet's now try and use regular expressions to extract the HTTP status codes from the logs"],"metadata":{"colab_type":"text","id":"-c-LK4TNXvAy","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab7f276f-de62-4f45-9c80-2847444b5980"}}},{"cell_type":"code","source":["status_pattern = r'\\s(\\d{3})\\s'\nstatus = [re.search(status_pattern, item).group(1) for item in sample_logs]\nprint(status)"],"metadata":{"colab_type":"code","id":"kvWWXeTcXvA1","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"172a981d-a94b-4564-a354-b73890913046"},"colab":{},"outputId":"5ffeabc8-ecff-4365-af5f-0b905f913115"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;200&#39;, &#39;304&#39;, &#39;304&#39;, &#39;304&#39;, &#39;304&#39;, &#39;200&#39;, &#39;304&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;304&#39;, &#39;200&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;200&#39;, &#39;304&#39;, &#39;304&#39;, &#39;304&#39;, &#39;304&#39;, &#39;200&#39;, &#39;304&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;200&#39;, &#39;304&#39;, &#39;200&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Extracting HTTP Response Content Size\n\nLet's now try and use regular expressions to extract the HTTP response content size from the logs"],"metadata":{"colab_type":"text","id":"jFemwBnSXvA4","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b62e61b7-f390-4c99-8fd5-d4ededcdcb14"}}},{"cell_type":"code","source":["content_size_pattern = r'\\s(\\d+)$'\ncontent_size = [re.search(content_size_pattern, item).group(1) for item in sample_logs]\nprint(content_size)"],"metadata":{"colab_type":"code","id":"l0kmQOqjXvA5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a60f3ac6-6d8f-4d75-a1d5-7d3745b7ba84"},"colab":{},"outputId":"8b6c6d90-6897-4885-e527-0b815f4a977e"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;1839&#39;, &#39;0&#39;, &#39;0&#39;, &#39;0&#39;, &#39;0&#39;, &#39;1713&#39;, &#39;0&#39;, &#39;1687&#39;, &#39;11853&#39;, &#39;9202&#39;, &#39;3635&#39;, &#39;1173&#39;, &#39;3047&#39;, &#39;0&#39;, &#39;10566&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;1839&#39;, &#39;0&#39;, &#39;0&#39;, &#39;0&#39;, &#39;0&#39;, &#39;1713&#39;, &#39;0&#39;, &#39;1687&#39;, &#39;11853&#39;, &#39;9202&#39;, &#39;3635&#39;, &#39;1173&#39;, &#39;3047&#39;, &#39;0&#39;, &#39;10566&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Q1: Your Turn: Putting it all together \n\nLet's now try and leverage all the regular expression patterns we previously built and use the `regexp_extract(...)` method to build our dataframe with all the log attributes neatly extracted in their own separate columns.\n\n- You can reuse the code we used previously to build the intermediate dataframe\n- Remember to cast the HTTP status code and content size as integers. \n- You can cast data as integer type using the following: __`regexp_extract('value', ...., ...).cast('integer').alias(...)`__"],"metadata":{"colab_type":"text","id":"DHPbsgMsXvA8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cae6f251-d04e-4dec-a21b-ab81a04ea1b3"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nlogs_df = base_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n                         regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n                         regexp_extract('value', status_pattern, 1).alias('status'),\n                         regexp_extract('value', content_size_pattern, 1).alias('content_size'))\nlogs_df.show(10, truncate=True)\nprint((logs_df.count(), len(logs_df.columns)))"],"metadata":{"colab_type":"code","id":"bM4iBWjRXvA-","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd759e94-fccd-4e5c-aac0-d1c543785380"},"colab":{},"outputId":"b826a364-3709-4c6f-eb67-541e08c535ac"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"logs_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------------+--------------------+------+--------------------+--------+------+------------+\n|                host|           timestamp|method|            endpoint|protocol|status|content_size|\n+--------------------+--------------------+------+--------------------+--------+------+------------+\n|   in24.inetnebr.com|01/Aug/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        1839|\n|     uplherc.upl.com|01/Aug/1995:00:00...|   GET|                   /|HTTP/1.0|   304|           0|\n|     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/ksclogo-m...|HTTP/1.0|   304|           0|\n|     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/MOSAIC-lo...|HTTP/1.0|   304|           0|\n|     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/USA-logos...|HTTP/1.0|   304|           0|\n|ix-esc-ca2-07.ix....|01/Aug/1995:00:00...|   GET|/images/launch-lo...|HTTP/1.0|   200|        1713|\n|     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/WORLD-log...|HTTP/1.0|   304|           0|\n|slppp6.intermind.net|01/Aug/1995:00:00...|   GET|/history/skylab/s...|HTTP/1.0|   200|        1687|\n|piweba4y.prodigy.com|01/Aug/1995:00:00...|   GET|/images/launchmed...|HTTP/1.0|   200|       11853|\n|slppp6.intermind.net|01/Aug/1995:00:00...|   GET|/history/skylab/s...|HTTP/1.0|   200|        9202|\n+--------------------+--------------------+------+--------------------+--------+------+------------+\nonly showing top 10 rows\n\n(3461613, 7)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+------+--------------------+--------+------+------------+\n                host|           timestamp|method|            endpoint|protocol|status|content_size|\n+--------------------+--------------------+------+--------------------+--------+------+------------+\n   in24.inetnebr.com|01/Aug/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        1839|\n     uplherc.upl.com|01/Aug/1995:00:00...|   GET|                   /|HTTP/1.0|   304|           0|\n     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/ksclogo-m...|HTTP/1.0|   304|           0|\n     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/MOSAIC-lo...|HTTP/1.0|   304|           0|\n     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/USA-logos...|HTTP/1.0|   304|           0|\nix-esc-ca2-07.ix....|01/Aug/1995:00:00...|   GET|/images/launch-lo...|HTTP/1.0|   200|        1713|\n     uplherc.upl.com|01/Aug/1995:00:00...|   GET|/images/WORLD-log...|HTTP/1.0|   304|           0|\nslppp6.intermind.net|01/Aug/1995:00:00...|   GET|/history/skylab/s...|HTTP/1.0|   200|        1687|\npiweba4y.prodigy.com|01/Aug/1995:00:00...|   GET|/images/launchmed...|HTTP/1.0|   200|       11853|\nslppp6.intermind.net|01/Aug/1995:00:00...|   GET|/history/skylab/s...|HTTP/1.0|   200|        9202|\n+--------------------+--------------------+------+--------------------+--------+------+------------+\nonly showing top 10 rows\n\n(3461613, 7)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Finding Missing Values\n\nMissing and null values are the bane of data analysis and machine learning. Let's see how well our data parsing and extraction logic worked. First, let's verify that there are no null rows in the original dataframe."],"metadata":{"colab_type":"text","id":"EgLGxYRYXvBB","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77d41255-ab1a-43d4-bb3c-478133549273"}}},{"cell_type":"code","source":["base_df.filter(base_df['value'].isNull()).count()"],"metadata":{"colab_type":"code","id":"O4ppVUoJXvBC","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56df1560-3723-49f0-be05-0b492015654b"},"colab":{},"outputId":"18c683c1-5f0d-4f62-9545-e9bd27088c80"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[20]: 0</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: 0</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If our data parsing and extraction worked properly, we should not have any rows with potential null values. Let's try and put that to test!"],"metadata":{"colab_type":"text","id":"7V0g6vHcXvBG","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53b7cc88-244c-4513-bb28-ab7555612d1a"}}},{"cell_type":"code","source":["bad_rows_df = logs_df.filter(logs_df['host'].isNull()| \n                             logs_df['timestamp'].isNull() | \n                             logs_df['method'].isNull() |\n                             logs_df['endpoint'].isNull() |\n                             logs_df['status'].isNull() |\n                             logs_df['content_size'].isNull()|\n                             logs_df['protocol'].isNull())\nbad_rows_df.count()"],"metadata":{"colab_type":"code","id":"4NFuk6QTXvBH","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9af6f80-0cd8-4485-b784-2e33511ad6a6"},"colab":{},"outputId":"a8238a74-51c8-48ac-fe83-b67f42d0a78c"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"bad_rows_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Out[21]: 0</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[21]: 0</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Ouch! Looks like we have over 30K missing values in our data! Can we handle this?"],"metadata":{"colab_type":"text","id":"CIqr5XvhXvBM","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abee8536-fbc5-40e6-baae-785176dc74dc"}}},{"cell_type":"markdown","source":["Do remember, this is not a regular pandas dataframe which you can directly query and get which columns have null. Our so-called _big dataset_ is residing on disk which can potentially be present in multiple nodes in a spark cluster. So how do we find out which columns have potential nulls? \n\n### Finding Null Counts\n\nWe can typically use the following technique to find out which columns have null values. \n\n(__Note:__ This approach is adapted from an [excellent answer](http://stackoverflow.com/a/33901312) on StackOverflow.)"],"metadata":{"colab_type":"text","id":"g8iWYrBoXvBN","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2914341-9d05-4e11-b2c6-788ed1ec119f"}}},{"cell_type":"code","source":["logs_df.columns"],"metadata":{"colab_type":"code","id":"h0TWdnaPXvBP","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47f0a636-ba66-4e5f-ab58-a08c113d7af8"},"colab":{},"outputId":"8a7abfda-2629-4f36-cb9c-3e00a13efa83"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[22]: [&#39;host&#39;,\n &#39;timestamp&#39;,\n &#39;method&#39;,\n &#39;endpoint&#39;,\n &#39;protocol&#39;,\n &#39;status&#39;,\n &#39;content_size&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[22]: [&#39;host&#39;,\n &#39;timestamp&#39;,\n &#39;method&#39;,\n &#39;endpoint&#39;,\n &#39;protocol&#39;,\n &#39;status&#39;,\n &#39;content_size&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.functions import sum as spark_sum\n\ndef count_null(col_name):\n    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n\n# Build up a list of column expressions, one per column.\nexprs = [count_null(col_name) for col_name in logs_df.columns]\n\n# Run the aggregation. The *exprs converts the list of expressions into\n# variable function arguments.\nlogs_df.agg(*exprs).show()"],"metadata":{"colab_type":"code","id":"LtNLdagZXvBT","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20671359-4327-4b87-95d3-10e418a85c51"},"colab":{},"outputId":"e9fba1ca-d183-4728-9f7c-135ebcd78ec9"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\n|host|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n|   0|        0|     0|       0|       0|     0|           0|\n+----+---------+------+--------+--------+------+------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\nhost|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n   0|        0|     0|       0|       0|     0|           0|\n+----+---------+------+--------+--------+------+------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Well, looks like we have one missing value in the `status` column and everything else is in the `content_size` column. \nLet's see if we can figure out what's wrong!"],"metadata":{"colab_type":"text","id":"g5z_B22MXvBX","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"395cd65a-6a62-412b-ab38-d7c6956497f7"}}},{"cell_type":"markdown","source":["### Handling nulls in HTTP status\n\nIf you had solved it correctly, our original parsing regular expression for the `status` column was:\n\n```\nregexp_extract('value', r'\\s(\\d{3})\\s', 1).cast('integer').alias('status')\n``` \n\nCould it be that there are more digits making our regular expression wrong? or is the data point itself bad? Let's try and find out!\n\n**Note**: In the expression below, `~` means \"not\"."],"metadata":{"colab_type":"text","id":"jGoLzGeEXvBZ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d39f0d1-6992-4f21-9464-78e4d4da7b5f"}}},{"cell_type":"code","source":["null_status_df = base_df.filter(~base_df['value'].rlike(r'\\s(\\d{3})\\s'))\nnull_status_df.count()"],"metadata":{"colab_type":"code","id":"tscbB01GXvBb","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b35d8fa3-483f-470a-9c05-cf5c736c518b"},"colab":{},"outputId":"97996e0a-5e21-441f-b742-3e898e691c43"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"null_status_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"value","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Out[24]: 1</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: 1</div>"]}}],"execution_count":0},{"cell_type":"code","source":["null_status_df.show(truncate=False)"],"metadata":{"colab_type":"code","id":"KMRiPoTXXvBe","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bf36ceb-18cf-4bff-b985-7c9c3031e536"},"colab":{},"outputId":"8bf8cf96-643f-42fd-c593-0dcadf950f74"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+\n|value   |\n+--------+\n|alyssa.p|\n+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\nvalue   |\n+--------+\nalyssa.p|\n+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["bad_status_df = null_status_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n                                      regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n                                      regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n                                      regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n                                      regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n                                      regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n                                      regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\nbad_status_df.show(truncate=False)"],"metadata":{"colab_type":"code","id":"-ec9HtGwXvBh","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0eea863-27b8-44b9-a36e-79ace75f341d"},"colab":{},"outputId":"9bc09049-3dff-4b65-8fec-5b7edf2cda39"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"bad_status_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"integer"},{"metadata":{},"name":"content_size","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\n|host|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n|    |         |      |        |        |null  |null        |\n+----+---------+------+--------+--------+------+------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\nhost|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n    |         |      |        |        |null  |null        |\n+----+---------+------+--------+--------+------+------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Looks like the record itself is an incomplete record with no useful information, the best option would be to drop this record as follows!"],"metadata":{"colab_type":"text","id":"wS7tZXIUXvBk","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"981e46e7-fa2f-4ba9-a134-f0c5e7c4d949"}}},{"cell_type":"code","source":["logs_df.count()"],"metadata":{"colab_type":"code","id":"zb6wxLWuXvBl","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d51a876-d980-45ed-be13-fb8bbe721d88"},"colab":{},"outputId":"b4f36a5a-9679-4830-91fc-80388aac54e2"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[27]: 3461613</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: 3461613</div>"]}}],"execution_count":0},{"cell_type":"code","source":["logs_df = logs_df[logs_df['status'].isNotNull()]\nlogs_df.count()"],"metadata":{"colab_type":"code","id":"RQogALLJXvBo","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b2162d5-8e68-42a4-b82d-b40ae76ee9cc"},"colab":{},"outputId":"41034dca-c862-48e7-9a3d-548738bebbff"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"logs_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Out[28]: 3461613</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[28]: 3461613</div>"]}}],"execution_count":0},{"cell_type":"code","source":["exprs = [count_null(col_name) for col_name in logs_df.columns]\nlogs_df.agg(*exprs).show()"],"metadata":{"colab_type":"code","id":"_rHsvCeeXvBs","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd307c41-d158-45a3-b1ed-d12040978739"},"colab":{},"outputId":"a58d88c2-94cc-4c58-bf1f-bf12abc0e822"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\n|host|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n|   0|        0|     0|       0|       0|     0|           0|\n+----+---------+------+--------+--------+------+------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\nhost|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n   0|        0|     0|       0|       0|     0|           0|\n+----+---------+------+--------+--------+------+------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Handling nulls in HTTP content size\n\nAgain based on our previous regular expression and assuming you were able to solve it correctly, our original parsing regular expression for the `content_size` column was:\n\n```\nregexp_extract('value', r'\\s(\\d+)$', 1).cast('integer').alias('content_size')\n``` \n\nCould there be missing data in our original dataset itself? Let's try and find out!"],"metadata":{"colab_type":"text","id":"8k4-PjEFXvBv","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05647f9d-0d07-42f1-b0d7-d758238093ac"}}},{"cell_type":"markdown","source":["### Q2: Your Turn: Find out the records in our base data frame with potential missing content sizes\n\n- Use the `r'\\s\\d+$'` regex pattern with the `rlike()` function like we demonstrated in the previous example\n- Remember to work on `base_df` since we are searching on the raw records NOT the parsed `logs_df`\n- Find the total count of the records with missing content size in `base_df` using the `count()` function"],"metadata":{"colab_type":"text","id":"MMjHfm1aXvBw","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6f371f5-3a03-4af8-9e9f-6e0803d61e5a"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nnull_content_size_df = base_df.filter(~base_df['value'].rlike(r'\\s\\d+$'))\nnull_content_size_df.count()"],"metadata":{"colab_type":"code","id":"ki7G6daTXvBx","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c617214-0a36-40ad-b6b7-24d4cec3d06e"},"colab":{},"outputId":"8a7230f3-f87b-4681-fcfc-b5d9503a441d"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"null_content_size_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"value","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Out[30]: 33905</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[30]: 33905</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Q3: Your Turn: Display the top ten records of your data frame having missing content sizes"],"metadata":{"colab_type":"text","id":"6te6fSfCXvB0","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a0904c9-962a-4e35-bab7-5a5134c6b4d2"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nnull_content_size_df.take(10)"],"metadata":{"colab_type":"code","id":"P1vwVWX4XvB1","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d4a0706-a9cb-41ec-9688-1540dda70ea3"},"colab":{},"outputId":"c11b52c6-ed73-421f-8988-8eb6296c7e7a"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: [Row(value=&#39;gw1.att.com - - [01/Aug/1995:00:03:53 -0400] &#34;GET /shuttle/missions/sts-73/news HTTP/1.0&#34; 302 -&#39;),\n Row(value=&#39;js002.cc.utsunomiya-u.ac.jp - - [01/Aug/1995:00:07:33 -0400] &#34;GET /shuttle/resources/orbiters/discovery.gif HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;tia1.eskimo.com - - [01/Aug/1995:00:28:41 -0400] &#34;GET /pub/winvn/release.txt HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;itws.info.eng.niigata-u.ac.jp - - [01/Aug/1995:00:38:01 -0400] &#34;GET /ksc.html/facts/about_ksc.html HTTP/1.0&#34; 403 -&#39;),\n Row(value=&#39;grimnet23.idirect.com - - [01/Aug/1995:00:50:12 -0400] &#34;GET /www/software/winvn/winvn.html HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;miriworld.its.unimelb.edu.au - - [01/Aug/1995:01:04:54 -0400] &#34;GET /history/history.htm HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;ras38.srv.net - - [01/Aug/1995:01:05:14 -0400] &#34;GET /elv/DELTA/uncons.htm HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;cs1-06.leh.ptd.net - - [01/Aug/1995:01:17:38 -0400] &#34;GET /sts-71/launch/&#34; 404 -&#39;),\n Row(value=&#39;www-b2.proxy.aol.com - - [01/Aug/1995:01:22:07 -0400] &#34;GET /shuttle/countdown HTTP/1.0&#34; 302 -&#39;),\n Row(value=&#39;maui56.maui.net - - [01/Aug/1995:01:31:56 -0400] &#34;GET /shuttle HTTP/1.0&#34; 302 -&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: [Row(value=&#39;gw1.att.com - - [01/Aug/1995:00:03:53 -0400] &#34;GET /shuttle/missions/sts-73/news HTTP/1.0&#34; 302 -&#39;),\n Row(value=&#39;js002.cc.utsunomiya-u.ac.jp - - [01/Aug/1995:00:07:33 -0400] &#34;GET /shuttle/resources/orbiters/discovery.gif HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;tia1.eskimo.com - - [01/Aug/1995:00:28:41 -0400] &#34;GET /pub/winvn/release.txt HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;itws.info.eng.niigata-u.ac.jp - - [01/Aug/1995:00:38:01 -0400] &#34;GET /ksc.html/facts/about_ksc.html HTTP/1.0&#34; 403 -&#39;),\n Row(value=&#39;grimnet23.idirect.com - - [01/Aug/1995:00:50:12 -0400] &#34;GET /www/software/winvn/winvn.html HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;miriworld.its.unimelb.edu.au - - [01/Aug/1995:01:04:54 -0400] &#34;GET /history/history.htm HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;ras38.srv.net - - [01/Aug/1995:01:05:14 -0400] &#34;GET /elv/DELTA/uncons.htm HTTP/1.0&#34; 404 -&#39;),\n Row(value=&#39;cs1-06.leh.ptd.net - - [01/Aug/1995:01:17:38 -0400] &#34;GET /sts-71/launch/&#34; 404 -&#39;),\n Row(value=&#39;www-b2.proxy.aol.com - - [01/Aug/1995:01:22:07 -0400] &#34;GET /shuttle/countdown HTTP/1.0&#34; 302 -&#39;),\n Row(value=&#39;maui56.maui.net - - [01/Aug/1995:01:31:56 -0400] &#34;GET /shuttle HTTP/1.0&#34; 302 -&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Assuming you were able to get to the missing records above, it is quite evident that the bad raw data records correspond to error responses, where no content was sent back and the server emitted a \"`-`\" for the `content_size` field. \n\nSince we don't want to discard those rows from our analysis, let's impute or fill them to 0."],"metadata":{"colab_type":"text","id":"Rl3J6c5sXvB8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"375fe457-38d5-433f-baf2-7a0d06c108a5"}}},{"cell_type":"markdown","source":["### Q4: Your Turn: Fix the rows with null content\\_size\n\nThe easiest solution is to replace the null values in `logs_df` with 0 like we discussed earlier. The Spark DataFrame API provides a set of functions and fields specifically designed for working with null values, among them:\n\n* [fillna()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna), which fills null values with specified non-null values.\n* [na](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na), which returns a [DataFrameNaFunctions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions) object with many functions for operating on null columns.\n\nThere are several ways to invoke this function. The easiest is just to replace _all_ null columns with known values. But, for safety, it's better to pass a Python dictionary containing (column\\_name, value) mappings. That's what we'll do. A sample example from the documentation is depicted below\n\n```\n>>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n+---+------+-------+\n|age|height|   name|\n+---+------+-------+\n| 10|    80|  Alice|\n|  5|  null|    Bob|\n| 50|  null|    Tom|\n| 50|  null|unknown|\n+---+------+-------+\n```\n\nNow use this function and fill all the missing values in the `content_size` field with 0!"],"metadata":{"colab_type":"text","id":"2y3iD47fXvCB","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"004170a8-a494-474d-86ce-000d65ac18a5"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nlogs_df = logs_df.na.fill({'content_size': 0})"],"metadata":{"colab_type":"code","collapsed":true,"id":"_2w5KTE0XvCC","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20de4584-c80f-4255-afe7-541730095d0a"},"colab":{},"outputId":"df4d371c-1376-4616-e2c7-9e35ddccf80e"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"logs_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"timestamp","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":false,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now assuming you were able to fill in the missing values successfully in the previous question, we should have no missing values \\ nulls in our dataset. Let's verify this!"],"metadata":{"colab_type":"text","id":"zdjkzs6UXvCJ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37c08bc9-e4a3-408b-82cf-ed0b51d59e4c"}}},{"cell_type":"code","source":["exprs = [count_null(col_name) for col_name in logs_df.columns]\nlogs_df.agg(*exprs).show()"],"metadata":{"colab_type":"code","id":"wDaLqVZOXvCK","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7022c129-bcb0-4c28-b14c-3cd5d332deec"},"colab":{},"outputId":"a3097bef-1d37-4213-a6e8-a52b08c27947"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\n|host|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n|   0|        0|     0|       0|       0|     0|           0|\n+----+---------+------+--------+--------+------+------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------+------+--------+--------+------+------------+\nhost|timestamp|method|endpoint|protocol|status|content_size|\n+----+---------+------+--------+--------+------+------------+\n   0|        0|     0|       0|       0|     0|           0|\n+----+---------+------+--------+--------+------+------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Look at that, no missing values!"],"metadata":{"colab_type":"text","id":"Bjr1f1m2XvCN","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69918436-3b5a-4d90-85a0-61f465147b98"}}},{"cell_type":"markdown","source":["## Handling Temporal Fields (Timestamp)\n\nNow that we have a clean, parsed DataFrame, we have to parse the timestamp field into an actual timestamp. The Common Log Format time is somewhat non-standard. A User-Defined Function (UDF) is the most straightforward way to parse it."],"metadata":{"colab_type":"text","id":"UFtYMGZhXvCN","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac3de413-80f6-45ac-8caf-9f0a5d4cdcc1"}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\nmonth_map = {\n  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n}\n\ndef parse_clf_time(text):\n    \"\"\" Convert Common Log time format into a Python datetime object\n    Args:\n        text (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n    Returns:\n        a string suitable for passing to CAST('timestamp')\n    \"\"\"\n    # NOTE: We're ignoring time zone here. In a production application, you'd want to handle that.\n    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n      int(text[7:11]),\n      month_map[text[3:6]],\n      int(text[0:2]),\n      int(text[12:14]),\n      int(text[15:17]),\n      int(text[18:20])\n    )"],"metadata":{"colab_type":"code","id":"k5t5VysnXvCS","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e75b4742-c0aa-4bf7-97f6-bb49bf5d5a79"},"colab":{},"outputId":"bebe2bb0-65ec-4d43-dbc9-24cda3c32de0"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sample_ts = [item['timestamp'] for item in logs_df.select('timestamp').take(5)]\nsample_ts"],"metadata":{"colab_type":"code","id":"RLJHPXHwXvCW","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f35a738-f598-4608-836f-4b4c71563e64"},"colab":{},"outputId":"2e58d673-3c1b-45fa-f283-886c698cfc6d"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[35]: [&#39;01/Aug/1995:00:00:01 -0400&#39;,\n &#39;01/Aug/1995:00:00:07 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[35]: [&#39;01/Aug/1995:00:00:01 -0400&#39;,\n &#39;01/Aug/1995:00:00:07 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;,\n &#39;01/Aug/1995:00:00:08 -0400&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["[parse_clf_time(item) for item in sample_ts]"],"metadata":{"colab_type":"code","id":"0K5Yd1Z2XvCY","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef13d379-18c7-4b4e-8c5e-3063446df238"},"colab":{},"outputId":"6108f5bb-bbd8-4c11-f9e4-efc491c543e0"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[36]: [&#39;1995-08-01 00:00:01&#39;,\n &#39;1995-08-01 00:00:07&#39;,\n &#39;1995-08-01 00:00:08&#39;,\n &#39;1995-08-01 00:00:08&#39;,\n &#39;1995-08-01 00:00:08&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: [&#39;1995-08-01 00:00:01&#39;,\n &#39;1995-08-01 00:00:07&#39;,\n &#39;1995-08-01 00:00:08&#39;,\n &#39;1995-08-01 00:00:08&#39;,\n &#39;1995-08-01 00:00:08&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["udf_parse_time = udf(parse_clf_time)\n\nlogs_df = logs_df.select('*', udf_parse_time(logs_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\nlogs_df.show(10, truncate=True)"],"metadata":{"colab_type":"code","id":"WBbQ_QNsXvCb","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96f16de8-c9ec-40ca-924d-2fe981a3f470"},"colab":{},"outputId":"b1e05731-4bff-410d-8c6f-d1637fd0f0a7"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"logs_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":false,"type":"string"},{"metadata":{},"name":"time","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------------+------+--------------------+--------+------+------------+-------------------+\n|                host|method|            endpoint|protocol|status|content_size|               time|\n+--------------------+------+--------------------+--------+------+------------+-------------------+\n|   in24.inetnebr.com|   GET|/shuttle/missions...|HTTP/1.0|   200|        1839|1995-08-01 00:00:01|\n|     uplherc.upl.com|   GET|                   /|HTTP/1.0|   304|           0|1995-08-01 00:00:07|\n|     uplherc.upl.com|   GET|/images/ksclogo-m...|HTTP/1.0|   304|           0|1995-08-01 00:00:08|\n|     uplherc.upl.com|   GET|/images/MOSAIC-lo...|HTTP/1.0|   304|           0|1995-08-01 00:00:08|\n|     uplherc.upl.com|   GET|/images/USA-logos...|HTTP/1.0|   304|           0|1995-08-01 00:00:08|\n|ix-esc-ca2-07.ix....|   GET|/images/launch-lo...|HTTP/1.0|   200|        1713|1995-08-01 00:00:09|\n|     uplherc.upl.com|   GET|/images/WORLD-log...|HTTP/1.0|   304|           0|1995-08-01 00:00:10|\n|slppp6.intermind.net|   GET|/history/skylab/s...|HTTP/1.0|   200|        1687|1995-08-01 00:00:10|\n|piweba4y.prodigy.com|   GET|/images/launchmed...|HTTP/1.0|   200|       11853|1995-08-01 00:00:10|\n|slppp6.intermind.net|   GET|/history/skylab/s...|HTTP/1.0|   200|        9202|1995-08-01 00:00:11|\n+--------------------+------+--------------------+--------+------+------------+-------------------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+--------------------+--------+------+------------+-------------------+\n                host|method|            endpoint|protocol|status|content_size|               time|\n+--------------------+------+--------------------+--------+------+------------+-------------------+\n   in24.inetnebr.com|   GET|/shuttle/missions...|HTTP/1.0|   200|        1839|1995-08-01 00:00:01|\n     uplherc.upl.com|   GET|                   /|HTTP/1.0|   304|           0|1995-08-01 00:00:07|\n     uplherc.upl.com|   GET|/images/ksclogo-m...|HTTP/1.0|   304|           0|1995-08-01 00:00:08|\n     uplherc.upl.com|   GET|/images/MOSAIC-lo...|HTTP/1.0|   304|           0|1995-08-01 00:00:08|\n     uplherc.upl.com|   GET|/images/USA-logos...|HTTP/1.0|   304|           0|1995-08-01 00:00:08|\nix-esc-ca2-07.ix....|   GET|/images/launch-lo...|HTTP/1.0|   200|        1713|1995-08-01 00:00:09|\n     uplherc.upl.com|   GET|/images/WORLD-log...|HTTP/1.0|   304|           0|1995-08-01 00:00:10|\nslppp6.intermind.net|   GET|/history/skylab/s...|HTTP/1.0|   200|        1687|1995-08-01 00:00:10|\npiweba4y.prodigy.com|   GET|/images/launchmed...|HTTP/1.0|   200|       11853|1995-08-01 00:00:10|\nslppp6.intermind.net|   GET|/history/skylab/s...|HTTP/1.0|   200|        9202|1995-08-01 00:00:11|\n+--------------------+------+--------------------+--------+------+------------+-------------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["logs_df.printSchema()"],"metadata":{"colab_type":"code","id":"9Yy-eIDtXvCi","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68ba2f4f-f4fd-4752-9b4c-3b80627b2239"},"colab":{},"outputId":"913a1185-f22d-4ac9-856b-d31db17eca28"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- host: string (nullable = true)\n |-- method: string (nullable = true)\n |-- endpoint: string (nullable = true)\n |-- protocol: string (nullable = true)\n |-- status: string (nullable = true)\n |-- content_size: string (nullable = false)\n |-- time: timestamp (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- host: string (nullable = true)\n-- method: string (nullable = true)\n-- endpoint: string (nullable = true)\n-- protocol: string (nullable = true)\n-- status: string (nullable = true)\n-- content_size: string (nullable = false)\n-- time: timestamp (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["logs_df.limit(5).toPandas()"],"metadata":{"colab_type":"code","id":"wfMB_2JvXvCk","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89d1a43e-d92c-49cb-81f9-779f3262ce9a"},"colab":{},"outputId":"dfb9673b-af0c-4232-d28f-674fa1662ac0"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>host</th>\n      <th>method</th>\n      <th>endpoint</th>\n      <th>protocol</th>\n      <th>status</th>\n      <th>content_size</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>in24.inetnebr.com</td>\n      <td>GET</td>\n      <td>/shuttle/missions/sts-68/news/sts-68-mcc-05.txt</td>\n      <td>HTTP/1.0</td>\n      <td>200</td>\n      <td>1839</td>\n      <td>1995-08-01 00:00:01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/images/ksclogo-medium.gif</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:08</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/images/MOSAIC-logosmall.gif</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:08</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/images/USA-logosmall.gif</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:08</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[39]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>host</th>\n      <th>method</th>\n      <th>endpoint</th>\n      <th>protocol</th>\n      <th>status</th>\n      <th>content_size</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>in24.inetnebr.com</td>\n      <td>GET</td>\n      <td>/shuttle/missions/sts-68/news/sts-68-mcc-05.txt</td>\n      <td>HTTP/1.0</td>\n      <td>200</td>\n      <td>1839</td>\n      <td>1995-08-01 00:00:01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/images/ksclogo-medium.gif</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:08</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/images/MOSAIC-logosmall.gif</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:08</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>uplherc.upl.com</td>\n      <td>GET</td>\n      <td>/images/USA-logosmall.gif</td>\n      <td>HTTP/1.0</td>\n      <td>304</td>\n      <td>0</td>\n      <td>1995-08-01 00:00:08</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's now cache `logs_df` since we will be using it extensively for our data analysis section in the next part!"],"metadata":{"colab_type":"text","id":"cn3ghlDsXvCp","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7b7a089-4b8b-4966-8aea-3bcc1acf90c4"}}},{"cell_type":"markdown","source":["# Step 2 - Exploratory Data Analysis on our Web Logs\n\nNow that we have a DataFrame containing the parsed log file as a data frame, we can perform some interesting exploratory data analysis (EDA)\n\n## Example: Content Size Statistics\n\nLet's compute some statistics about the sizes of content being returned by the web server. In particular, we'd like to know what are the average, minimum, and maximum content sizes.\n\nWe can compute the statistics by calling `.describe()` on the `content_size` column of `logs_df`.  The `.describe()` function returns the count, mean, stddev, min, and max of a given column."],"metadata":{"colab_type":"text","id":"1WTHkvANXvCq","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83f651ff-7cac-45e9-9ee3-4a946039ae88"}}},{"cell_type":"code","source":["content_size_summary_df = logs_df.describe(['content_size'])\ncontent_size_summary_df.toPandas()"],"metadata":{"colab_type":"code","id":"FHkVy3ZXXvCr","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4aa3cf27-ddfb-4edb-b7dd-7c8c073a1c6e"},"colab":{},"outputId":"cd083d90-c450-4d64-9d56-c8e1f5dd1eed"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"content_size_summary_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"summary","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>content_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>3461613</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>19116.072581153352</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stddev</td>\n      <td>73367.37951430604</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>max</td>\n      <td>99981</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[40]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>content_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>3461613</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>19116.072581153352</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stddev</td>\n      <td>73367.37951430604</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>max</td>\n      <td>99981</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Alternatively, we can use SQL to directly calculate these statistics.  You can explore many useful functions within the `pyspark.sql.functions` module in the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions).\n\nAfter we apply the `.agg()` function, we call `toPandas()` to extract and convert the result into a `pandas` dataframe which has better formatting on Jupyter notebooks"],"metadata":{"colab_type":"text","id":"fY1S9FPAXvCv","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e66c9c4-db4a-4db1-906d-06ba9ce4569a"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\n(logs_df.agg(F.min(logs_df['content_size']).alias('min_content_size'),\n             F.max(logs_df['content_size']).alias('max_content_size'),\n             F.mean(logs_df['content_size']).alias('mean_content_size'),\n             F.stddev(logs_df['content_size']).alias('std_content_size'),\n             F.count(logs_df['content_size']).alias('count_content_size'))\n        .toPandas())"],"metadata":{"colab_type":"code","id":"nonAax8AXvCx","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c25564cb-ba0c-4413-8caf-ae80cbd24a93"},"colab":{},"outputId":"1a9a01ac-35f3-44ab-f105-5759f8f79b9a"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_content_size</th>\n      <th>max_content_size</th>\n      <th>mean_content_size</th>\n      <th>std_content_size</th>\n      <th>count_content_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>99981</td>\n      <td>19116.072581</td>\n      <td>73367.379514</td>\n      <td>3461613</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[41]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_content_size</th>\n      <th>max_content_size</th>\n      <th>mean_content_size</th>\n      <th>std_content_size</th>\n      <th>count_content_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>99981</td>\n      <td>19116.072581</td>\n      <td>73367.379514</td>\n      <td>3461613</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Example: HTTP Status Code Analysis\n\nNext, let's look at the status code values that appear in the log. We want to know which status code values appear in the data and how many times.  \n\nWe again start with `logs_df`, then group by the `status` column, apply the `.count()` aggregation function, and sort by the `status` column."],"metadata":{"colab_type":"text","id":"8hat3HPwXvC0","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fbbcd74-960a-487a-9305-f569f24643f5"}}},{"cell_type":"code","source":["status_freq_df = (logs_df\n                     .groupBy('status')\n                     .count()\n                     .sort('status')\n                     .cache())"],"metadata":{"colab_type":"code","collapsed":true,"id":"jL-vEt0FXvC1","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b41118d-5581-4e42-a0f8-aa3bc11883bd"},"colab":{},"outputId":"8ee8079f-8b03-4e70-8335-cbd7b2d0b1bc"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"status_freq_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print('Total distinct HTTP Status Codes:', status_freq_df.count())"],"metadata":{"colab_type":"code","id":"a0iPsCW2XvC3","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f8090f9-fa32-41e1-bf3c-628766c299ca"},"colab":{},"outputId":"64fbd3f8-cf08-4ca3-de33-32349d3d6597"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Total distinct HTTP Status Codes: 9\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total distinct HTTP Status Codes: 9\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["status_freq_pd_df = status_freq_df.toPandas()\nstatus_freq_pd_df"],"metadata":{"colab_type":"code","id":"SZO7ncjdXvC6","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c090e40-572f-44db-813f-ab0d0c81fd79"},"colab":{},"outputId":"0344aef8-367b-476b-fc2c-e6c1346d3da9"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>status</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>3100524</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>302</td>\n      <td>73070</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>304</td>\n      <td>266773</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>400</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>403</td>\n      <td>225</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>404</td>\n      <td>20899</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>500</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>501</td>\n      <td>41</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[44]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>status</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>3100524</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>302</td>\n      <td>73070</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>304</td>\n      <td>266773</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>400</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>403</td>\n      <td>225</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>404</td>\n      <td>20899</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>500</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>501</td>\n      <td>41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n%matplotlib inline\n\nstatus_freq_pd_df.plot(x='status', y='count', kind='bar')"],"metadata":{"colab":{},"colab_type":"code","id":"KXkKQA-WXvC8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30da3fda-858c-4823-88eb-9120d487e49d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/ee3dfcc6-14a0-4b77-8a2a-73623ede82eb.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}}],"execution_count":0},{"cell_type":"code","source":["log_freq_df = status_freq_df.withColumn('log(count)', F.log(status_freq_df['count']))\nlog_freq_df.show()"],"metadata":{"colab_type":"code","id":"iabid8S6XvC_","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0403509-03f2-4556-b508-4dc93c1999f0"},"colab":{},"outputId":"31df7ac0-35f9-4f9e-f059-c4267865c9b6"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"log_freq_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"},{"metadata":{},"name":"log(count)","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+------+-------+------------------+\n|status|  count|        log(count)|\n+------+-------+------------------+\n|      |      1|               0.0|\n|   200|3100524|14.947081687429097|\n|   302|  73070|11.199173164785263|\n|   304| 266773|12.494153388502301|\n|   400|     15|  2.70805020110221|\n|   403|    225|  5.41610040220442|\n|   404|  20899| 9.947456589918252|\n|   500|     65| 4.174387269895637|\n|   501|     41| 3.713572066704308|\n+------+-------+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-------+------------------+\nstatus|  count|        log(count)|\n+------+-------+------------------+\n      |      1|               0.0|\n   200|3100524|14.947081687429097|\n   302|  73070|11.199173164785263|\n   304| 266773|12.494153388502301|\n   400|     15|  2.70805020110221|\n   403|    225|  5.41610040220442|\n   404|  20899| 9.947456589918252|\n   500|     65| 4.174387269895637|\n   501|     41| 3.713572066704308|\n+------+-------+------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Q5: Your Turn: Convert the log\\_freq\\_df to a pandas DataFrame and plot a bar chart displaying counts of each HTTP Status Code"],"metadata":{"colab_type":"text","id":"CWcfTjlUXvDC","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e16111b-81ef-4a6b-bd03-1cf2f013cd1c"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nlog_freq_pd_df = log_freq_df.toPandas()\nlog_freq_pd_df.plot(x=\"status\", y=\"count\", kind=\"bar\")"],"metadata":{"colab":{},"colab_type":"code","id":"RWWwF5NcXvDD","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b05abf-189c-4f8b-86f0-6269fa157d33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/0411d9e0-b4c0-4a3a-8dec-6dfc2ee592e1.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn: Q6: Analyzing Frequent Hosts\n\nLet's look at hosts that have accessed the server frequently. Try to get the count of total accesses by each `host` and then sort by the counts and display only the top ten most frequent hosts.\n\n__Hints:__\n\n- Your Spark DataFrame has a `host` column\n- Get the counts per `host` which would make a `count` column\n- Sort by the counts. Please check [__the documentation__](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sort) to see how to sort in reverse\n- Remember only to get the top 10 rows from the aggregated dataframe and show them"],"metadata":{"colab_type":"text","id":"k5oXMLxSXvDH","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89785f98-a93f-4f13-9019-9b69edbe507a"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nhost_sum_df =(logs_df.groupBy('host')\n                     .count()\n                     .sort('count', ascending = False)\n                     .limit(10))\n\nhost_sum_df.show(truncate=False)"],"metadata":{"colab_type":"code","id":"eBBayokUXvDM","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df0456ad-882e-4cff-bccc-b380e6438ea0"},"colab":{},"outputId":"83a26ad7-21c2-4b15-b527-5de9ac5b8c5c"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"host_sum_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+--------------------+-----+\n|host                |count|\n+--------------------+-----+\n|piweba3y.prodigy.com|21988|\n|piweba4y.prodigy.com|16437|\n|piweba1y.prodigy.com|12825|\n|edams.ksc.nasa.gov  |11964|\n|163.206.89.4        |9697 |\n|news.ti.com         |8161 |\n|www-d1.proxy.aol.com|8047 |\n|alyssa.prodigy.com  |8037 |\n|                    |7661 |\n|siltb10.orl.mmc.com |7573 |\n+--------------------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-----+\nhost                |count|\n+--------------------+-----+\npiweba3y.prodigy.com|21988|\npiweba4y.prodigy.com|16437|\npiweba1y.prodigy.com|12825|\nedams.ksc.nasa.gov  |11964|\n163.206.89.4        |9697 |\nnews.ti.com         |8161 |\nwww-d1.proxy.aol.com|8047 |\nalyssa.prodigy.com  |8037 |\n                    |7661 |\nsiltb10.orl.mmc.com |7573 |\n+--------------------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["host_sum_pd_df = host_sum_df.toPandas()\nhost_sum_pd_df.iloc[8]['host']"],"metadata":{"colab_type":"code","id":"1dv8Ny3iXvDQ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"073afa5c-24b1-4597-b2d9-168620ed50da"},"colab":{},"outputId":"ea532f95-2ff4-4e36-abc5-36297902d8e6"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[49]: &#39;&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: &#39;&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Looks like we have some empty strings as one of the top host names! This teaches us a valuable lesson to not just check for nulls but also potentially empty strings when data wrangling."],"metadata":{"colab_type":"text","id":"qJNpJU2VXvDT","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cfb4591-9b53-41ef-9cb8-8e84aea94ad6"}}},{"cell_type":"markdown","source":["## Your Turn: Q7: Display the Top 20 Frequent EndPoints\n\nNow, let's visualize the number of hits to endpoints (URIs) in the log. To perform this task, start with our `logs_df` and group by the `endpoint` column, aggregate by count, and sort in descending order like the previous question. Also remember to show only the top 20 most frequently accessed endpoints"],"metadata":{"colab_type":"text","id":"Pq5jiBtIXvDU","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d544a4be-9257-4c90-a4f2-50b176ece4a3"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\npaths_df = (logs_df.groupby('endpoint')\n                   .count()\n                   .sort(\"count\", ascending = False)\n                   .limit(20)\n           )"],"metadata":{"colab_type":"code","id":"oxZRXX-AXvDU","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03d6b2c1-8d96-4317-b792-f6e1219e68e8"},"colab":{},"outputId":"bae76bd6-6f67-478c-ea46-928f545cf4bc"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"paths_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["paths_pd_df = paths_df.toPandas()\npaths_pd_df"],"metadata":{"colab_type":"code","id":"DwfjJG2jXvDX","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60713f9e-d6ef-419c-914f-5c9ca798aefd"},"colab":{},"outputId":"d44c9385-5a6b-4157-83bc-d4d6832cc6a6"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>endpoint</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/images/NASA-logosmall.gif</td>\n      <td>208714</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/images/KSC-logosmall.gif</td>\n      <td>164970</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/images/MOSAIC-logosmall.gif</td>\n      <td>127908</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/images/USA-logosmall.gif</td>\n      <td>127074</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/images/WORLD-logosmall.gif</td>\n      <td>125925</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/images/ksclogo-medium.gif</td>\n      <td>121572</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/ksc.html</td>\n      <td>83909</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/images/launch-logo.gif</td>\n      <td>76006</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/history/apollo/images/apollo-logo1.gif</td>\n      <td>68896</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/shuttle/countdown/</td>\n      <td>64736</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>/</td>\n      <td>63171</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>/images/ksclogosmall.gif</td>\n      <td>61393</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>/shuttle/missions/missions.html</td>\n      <td>47315</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>/images/launchmedium.gif</td>\n      <td>40687</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>/htbin/cdt_main.pl</td>\n      <td>39871</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>/shuttle/missions/sts-69/mission-sts-69.html</td>\n      <td>31574</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>/shuttle/countdown/liftoff.html</td>\n      <td>29865</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>/icons/menu.xbm</td>\n      <td>29190</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>/shuttle/missions/sts-69/sts-69-patch-small.gif</td>\n      <td>29118</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>/icons/blank.xbm</td>\n      <td>28852</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[51]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>endpoint</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/images/NASA-logosmall.gif</td>\n      <td>208714</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/images/KSC-logosmall.gif</td>\n      <td>164970</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/images/MOSAIC-logosmall.gif</td>\n      <td>127908</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/images/USA-logosmall.gif</td>\n      <td>127074</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/images/WORLD-logosmall.gif</td>\n      <td>125925</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/images/ksclogo-medium.gif</td>\n      <td>121572</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/ksc.html</td>\n      <td>83909</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/images/launch-logo.gif</td>\n      <td>76006</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/history/apollo/images/apollo-logo1.gif</td>\n      <td>68896</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/shuttle/countdown/</td>\n      <td>64736</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>/</td>\n      <td>63171</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>/images/ksclogosmall.gif</td>\n      <td>61393</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>/shuttle/missions/missions.html</td>\n      <td>47315</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>/images/launchmedium.gif</td>\n      <td>40687</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>/htbin/cdt_main.pl</td>\n      <td>39871</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>/shuttle/missions/sts-69/mission-sts-69.html</td>\n      <td>31574</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>/shuttle/countdown/liftoff.html</td>\n      <td>29865</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>/icons/menu.xbm</td>\n      <td>29190</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>/shuttle/missions/sts-69/sts-69-patch-small.gif</td>\n      <td>29118</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>/icons/blank.xbm</td>\n      <td>28852</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn: Q8: Top Ten Error Endpoints\n\nWhat are the top ten endpoints requested which did not have return code 200 (HTTP Status OK)? \n\nCreate a sorted list containing the endpoints and the number of times that they were accessed with a non-200 return code and show the top ten.\n\nThink about the steps that you need to perform to determine which endpoints did not have a 200 return code (combination of filtering, grouping, sorting and selecting the top ten aggregated records)"],"metadata":{"colab_type":"text","id":"LrK4j5aIXvDb","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c9e6c04-ee2d-4ebf-96e2-bcd6ca6a8e15"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nnot200_df = (logs_df.filter(logs_df[\"status\"] != 200))\n\nerror_endpoints_freq_df = (not200_df\n                               .groupby(\"endpoint\")\n                               .count()\n                               .sort(\"count\", ascending = False)\n                               .limit(10)\n                          )"],"metadata":{"colab_type":"code","id":"s57_3ODMXvDc","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abeedad5-206b-4c54-87d7-0b1522e39a59"},"colab":{},"outputId":"d25e0c40-eaff-459a-9fd4-40666e226d28"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"not200_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":false,"type":"string"},{"metadata":{},"name":"time","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null},{"name":"error_endpoints_freq_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["error_endpoints_freq_df.show(truncate=False)"],"metadata":{"colab_type":"code","id":"GeAUAEMBXvDh","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"098c14a3-a01f-4241-a0f4-311385f96c37"},"colab":{},"outputId":"a5bc7840-a464-47ae-f014-9e3108c2b306"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------------------------------+-----+\n|endpoint                               |count|\n+---------------------------------------+-----+\n|/images/NASA-logosmall.gif             |40082|\n|/images/KSC-logosmall.gif              |23763|\n|/images/MOSAIC-logosmall.gif           |15245|\n|/images/USA-logosmall.gif              |15142|\n|/images/WORLD-logosmall.gif            |14773|\n|/images/ksclogo-medium.gif             |13559|\n|/images/launch-logo.gif                |8806 |\n|/history/apollo/images/apollo-logo1.gif|7489 |\n|/                                      |6296 |\n|/images/ksclogosmall.gif               |5669 |\n+---------------------------------------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------+-----+\nendpoint                               |count|\n+---------------------------------------+-----+\n/images/NASA-logosmall.gif             |40082|\n/images/KSC-logosmall.gif              |23763|\n/images/MOSAIC-logosmall.gif           |15245|\n/images/USA-logosmall.gif              |15142|\n/images/WORLD-logosmall.gif            |14773|\n/images/ksclogo-medium.gif             |13559|\n/images/launch-logo.gif                |8806 |\n/history/apollo/images/apollo-logo1.gif|7489 |\n/                                      |6296 |\n/images/ksclogosmall.gif               |5669 |\n+---------------------------------------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Example: Number of Unique Daily Hosts\n\nFor an advanced example, let's look at a way to determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts. \n\nWe'd like a DataFrame sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day. \n\nThink about the steps that you need to perform to count the number of different hosts that make requests *each* day.\n*Since the log only covers a single month, you can ignore the month.*  You may want to use the [`dayofmonth` function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth) in the `pyspark.sql.functions` module (which we have already imported as __`F`__.\n\n\n**`host_day_df`**\n\nA DataFrame with two columns\n\n| column | explanation          |\n| ------ | -------------------- |\n| `host` | the host name        |\n| `day`  | the day of the month |\n\nThere will be one row in this DataFrame for each row in `logs_df`. Essentially, we are just transforming each row of `logs_df`. For example, for this row in `logs_df`:\n\n```\nunicomp6.unicomp.net - - [01/Aug/1995:00:35:41 -0400] \"GET /shuttle/missions/sts-73/news HTTP/1.0\" 302 -\n```\n\nyour `host_day_df` should have:\n\n```\nunicomp6.unicomp.net 1\n```"],"metadata":{"colab_type":"text","id":"ij1wJnMwXvDm","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"112856ff-2e0c-4fcc-8731-bb5ecaf54358"}}},{"cell_type":"code","source":["host_day_df = logs_df.select(logs_df.host, \n                             F.dayofmonth('time').alias('day'))\nhost_day_df.show(5, truncate=False)"],"metadata":{"colab_type":"code","id":"c0i7-gm9XvDm","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0bc0a53-c4f1-4846-ac0a-216586709e2f"},"colab":{},"outputId":"ef93f5a2-e15d-4631-cbb7-e390dd0c28f0"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"host_day_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"day","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------+---+\n|host             |day|\n+-----------------+---+\n|in24.inetnebr.com|1  |\n|uplherc.upl.com  |1  |\n|uplherc.upl.com  |1  |\n|uplherc.upl.com  |1  |\n|uplherc.upl.com  |1  |\n+-----------------+---+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+---+\nhost             |day|\n+-----------------+---+\nin24.inetnebr.com|1  |\nuplherc.upl.com  |1  |\nuplherc.upl.com  |1  |\nuplherc.upl.com  |1  |\nuplherc.upl.com  |1  |\n+-----------------+---+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**`host_day_distinct_df`**\n\nThis DataFrame has the same columns as `host_day_distinct_df`, but with duplicate (`day`, `host`) rows removed."],"metadata":{"colab_type":"text","id":"HRu_5V4bXvDs","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57581faf-41f6-4313-a0c7-c3831bbeea7a"}}},{"cell_type":"code","source":["host_day_distinct_df = (host_day_df\n                          .dropDuplicates())\nhost_day_distinct_df.show(5, truncate=False)"],"metadata":{"colab_type":"code","id":"be9KD86vXvDt","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ccf36aa-b9fb-47a3-8ed5-7ff205bed736"},"colab":{},"outputId":"013470c7-93ab-4c4a-945a-af4c212c85d7"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"host_day_distinct_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"day","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-------------------------+---+\n|host                     |day|\n+-------------------------+---+\n|193.104.168.105          |29 |\n|slip152-238.on.ca.ibm.net|19 |\n|als.sdrborges.dk         |29 |\n|ts2-025.jaxnet.com       |12 |\n|www-c6.proxy.aol.com     |19 |\n+-------------------------+---+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------+---+\nhost                     |day|\n+-------------------------+---+\n193.104.168.105          |29 |\nslip152-238.on.ca.ibm.net|19 |\nals.sdrborges.dk         |29 |\nts2-025.jaxnet.com       |12 |\nwww-c6.proxy.aol.com     |19 |\n+-------------------------+---+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**`daily_unique_hosts_df`**\n\nA DataFrame with two columns:\n\n| column  | explanation                                        |\n| ------- | -------------------------------------------------- |\n| `day`   | the day of the month                               |\n| `count` | the number of unique requesting hosts for that day |"],"metadata":{"colab_type":"text","id":"OlCEA952XvDv","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a65e1b10-1de6-4dd5-9c0f-aa821d1a443d"}}},{"cell_type":"code","source":["daily_hosts_df = (host_day_distinct_df\n                     .groupBy('day')\n                     .count()\n                     .sort(\"day\"))\ndaily_hosts_df = daily_hosts_df.toPandas()\ndaily_hosts_df.T"],"metadata":{"colab_type":"code","id":"TrVQJOKPXvDv","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2eb3328a-5138-4fe3-9025-e1061b14866f"},"colab":{},"outputId":"875f282f-13f6-4f9c-b4ee-a8f867f2955f"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/dataframe.py:2200: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.enabled&#39; is set to true, but has reached the error below and can not continue. Note that &#39;spark.sql.execution.arrow.fallback.enabled&#39; does not have an effect on failures in the middle of computation.\n  An error occurred while calling o3199.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 282.0 failed 1 times, most recent failure: Lost task 7.0 in stage 282.0 (TID 8380, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2282)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2304)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2348)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:232)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.Dataset.toArrowBatchRdd(Dataset.scala:3557)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/dataframe.py:2200: UserWarning: toPandas attempted Arrow optimization because &#39;spark.sql.execution.arrow.enabled&#39; is set to true, but has reached the error below and can not continue. Note that &#39;spark.sql.execution.arrow.fallback.enabled&#39; does not have an effect on failures in the middle of computation.\n  An error occurred while calling o3199.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 282.0 failed 1 times, most recent failure: Lost task 7.0 in stage 282.0 (TID 8380, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2282)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2304)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2348)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:232)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.Dataset.toArrowBatchRdd(Dataset.scala:3557)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n  warnings.warn(msg)\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033903&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>                      <span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                      .sort(&#34;day&#34;))\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>daily_hosts_df <span class=\"ansi-blue-fg\">=</span> daily_hosts_df<span class=\"ansi-blue-fg\">.</span>toPandas<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> daily_hosts_df<span class=\"ansi-blue-fg\">.</span>T\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">toPandas</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2182</span>                         _check_dataframe_localize_timestamps\n<span class=\"ansi-green-intense-fg ansi-bold\">   2183</span>                     <span class=\"ansi-green-fg\">import</span> pyarrow\n<span class=\"ansi-green-fg\">-&gt; 2184</span><span class=\"ansi-red-fg\">                     </span>batches <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_collectAsArrow<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2185</span>                     <span class=\"ansi-green-fg\">if</span> len<span class=\"ansi-blue-fg\">(</span>batches<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2186</span>                         table <span class=\"ansi-blue-fg\">=</span> pyarrow<span class=\"ansi-blue-fg\">.</span>Table<span class=\"ansi-blue-fg\">.</span>from_batches<span class=\"ansi-blue-fg\">(</span>batches<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">_collectAsArrow</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2245</span>                 <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>port<span class=\"ansi-blue-fg\">,</span> auth_secret<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> ArrowStreamSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2246</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 2247</span><span class=\"ansi-red-fg\">                 </span>jsocket_auth_server<span class=\"ansi-blue-fg\">.</span>getResult<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># Join serving thread and raise any exceptions</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2248</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2249</span>     <span class=\"ansi-red-fg\">##########################################################################################</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3199.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 282.0 failed 1 times, most recent failure: Lost task 7.0 in stage 282.0 (TID 8380, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2282)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2304)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2348)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:232)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.Dataset.toArrowBatchRdd(Dataset.scala:3557)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>","errorSummary":"org.apache.spark.SparkException: Exception thrown in awaitResult: ","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033903&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>                      <span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                      .sort(&#34;day&#34;))\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>daily_hosts_df <span class=\"ansi-blue-fg\">=</span> daily_hosts_df<span class=\"ansi-blue-fg\">.</span>toPandas<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> daily_hosts_df<span class=\"ansi-blue-fg\">.</span>T\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">toPandas</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2182</span>                         _check_dataframe_localize_timestamps\n<span class=\"ansi-green-intense-fg ansi-bold\">   2183</span>                     <span class=\"ansi-green-fg\">import</span> pyarrow\n<span class=\"ansi-green-fg\">-&gt; 2184</span><span class=\"ansi-red-fg\">                     </span>batches <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_collectAsArrow<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2185</span>                     <span class=\"ansi-green-fg\">if</span> len<span class=\"ansi-blue-fg\">(</span>batches<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2186</span>                         table <span class=\"ansi-blue-fg\">=</span> pyarrow<span class=\"ansi-blue-fg\">.</span>Table<span class=\"ansi-blue-fg\">.</span>from_batches<span class=\"ansi-blue-fg\">(</span>batches<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">_collectAsArrow</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2245</span>                 <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>port<span class=\"ansi-blue-fg\">,</span> auth_secret<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> ArrowStreamSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2246</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 2247</span><span class=\"ansi-red-fg\">                 </span>jsocket_auth_server<span class=\"ansi-blue-fg\">.</span>getResult<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># Join serving thread and raise any exceptions</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2248</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2249</span>     <span class=\"ansi-red-fg\">##########################################################################################</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3199.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:358)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:67)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 282.0 failed 1 times, most recent failure: Lost task 7.0 in stage 282.0 (TID 8380, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2282)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2304)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2348)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:232)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.Dataset.toArrowBatchRdd(Dataset.scala:3557)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$20.apply(Dataset.scala:3409)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply$mcV$sp(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6$$anonfun$apply$7.apply(PythonRDD.scala:627)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:628)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:624)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1172)\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:1166)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1$$anonfun$apply$1.apply(SocketAuthServer.scala:48)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:48)\n\tat org.apache.spark.security.SocketAuthServer$$anonfun$1.apply(SocketAuthServer.scala:47)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:102)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 460, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 150, in dump_stream\n    for obj in iterator:\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 449, in _batched\n    for item in iterator:\n  File &#34;&lt;string&gt;&#34;, line 1, in &lt;lambda&gt;\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 87, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-3632668296033869&gt;&#34;, line 17, in parse_clf_time\nValueError: invalid literal for int() with base 10: &#39;&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:552)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:558)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["daily_hosts_df.plot(x='day', y='count', kind='line')"],"metadata":{"colab":{},"colab_type":"code","id":"PUdqNQHiXvDz","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"010e7e89-5d8e-428c-963f-26d64603b15b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033904&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>daily_hosts_df<span class=\"ansi-blue-fg\">.</span>plot<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;day&#39;</span><span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;count&#39;</span><span class=\"ansi-blue-fg\">,</span> kind<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;line&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1332</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1333</span>             raise AttributeError(\n<span class=\"ansi-green-fg\">-&gt; 1334</span><span class=\"ansi-red-fg\">                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1335</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1336</span>         <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;plot&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;plot&#39;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033904&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>daily_hosts_df<span class=\"ansi-blue-fg\">.</span>plot<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;day&#39;</span><span class=\"ansi-blue-fg\">,</span> y<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;count&#39;</span><span class=\"ansi-blue-fg\">,</span> kind<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;line&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1332</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1333</span>             raise AttributeError(\n<span class=\"ansi-green-fg\">-&gt; 1334</span><span class=\"ansi-red-fg\">                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1335</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1336</span>         <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;plot&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn: Q9: Counting 404 Response Codes\n\nCreate a DataFrame containing only log records with a 404 status code (Not Found). \n\nMake sure you `cache()` the `not_found_df` dataframe as we will use it in the rest of the exercises here.\n\n__How many 404 records are in the log?__"],"metadata":{"colab_type":"text","id":"O5M-HoWHXvD1","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"037ad98a-9d25-4243-acf0-2b542dad1396"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nnot_found_df = logs_df.filter(logs_df[\"status\"] == 404).cache()\nprint(('Total 404 responses: {}').format(not_found_df.count()))"],"metadata":{"colab_type":"code","id":"XovkxrduXvD2","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46dfcb25-e4ca-4381-b9c0-f8a083c762db"},"colab":{},"outputId":"662c54ce-9fd3-4d97-dbf5-30a1a77b90f9"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"not_found_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"host","nullable":true,"type":"string"},{"metadata":{},"name":"method","nullable":true,"type":"string"},{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"protocol","nullable":true,"type":"string"},{"metadata":{},"name":"status","nullable":true,"type":"string"},{"metadata":{},"name":"content_size","nullable":false,"type":"string"},{"metadata":{},"name":"time","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Total 404 responses: 20899\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total 404 responses: 20899\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn: Q10: Listing the Top Twenty 404 Response Code Endpoints\n\nUsing the DataFrame containing only log records with a 404 response code that you cached in Q9, print out a list of the top twenty endpoints that generate the most 404 errors.\n\n*Remember, top endpoints should be in sorted order*"],"metadata":{"colab_type":"text","id":"wdwYDLCnXvD5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4da0b9cf-f451-46bf-adca-f4527cc957f8"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nhosts_404_count_df = (not_found_df\n                          .groupby(\"endpoint\")\n                          .count()\n                          .sort('count', ascending = False)\n                          .limit(20))\n\nhosts_404_count_df.show(truncate=False)"],"metadata":{"colab_type":"code","id":"Gv3egQAEXvD5","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b19e93d-f7ac-4588-a733-b6c7aca3dbde"},"colab":{},"outputId":"6725f87c-1cd4-4387-f61f-9c7cd58f1f93"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"hosts_404_count_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"endpoint","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------------------------------------------------------+-----+\n|endpoint                                                         |count|\n+-----------------------------------------------------------------+-----+\n|/pub/winvn/readme.txt                                            |2004 |\n|/pub/winvn/release.txt                                           |1732 |\n|/shuttle/missions/STS-69/mission-STS-69.html                     |683  |\n|/shuttle/missions/sts-68/ksc-upclose.gif                         |428  |\n|/history/apollo/a-001/a-001-patch-small.gif                      |384  |\n|/history/apollo/sa-1/sa-1-patch-small.gif                        |383  |\n|/://spacelink.msfc.nasa.gov                                      |381  |\n|/images/crawlerway-logo.gif                                      |374  |\n|/elv/DELTA/uncons.htm                                            |372  |\n|/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif|359  |\n|/images/nasa-logo.gif                                            |319  |\n|/shuttle/resources/orbiters/atlantis.gif                         |314  |\n|/history/apollo/apollo-13.html                                   |304  |\n|/shuttle/resources/orbiters/discovery.gif                        |263  |\n|/shuttle/missions/sts-71/images/KSC-95EC-0916.txt                |190  |\n|/shuttle/resources/orbiters/challenger.gif                       |170  |\n|/shuttle/missions/technology/sts-newsref/stsref-toc.html         |158  |\n|/history/apollo/images/little-joe.jpg                            |150  |\n|/images/lf-logo.gif                                              |143  |\n|/history/apollo/publications/sp-350/sp-350.txt~                  |140  |\n+-----------------------------------------------------------------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------+-----+\nendpoint                                                         |count|\n+-----------------------------------------------------------------+-----+\n/pub/winvn/readme.txt                                            |2004 |\n/pub/winvn/release.txt                                           |1732 |\n/shuttle/missions/STS-69/mission-STS-69.html                     |683  |\n/shuttle/missions/sts-68/ksc-upclose.gif                         |428  |\n/history/apollo/a-001/a-001-patch-small.gif                      |384  |\n/history/apollo/sa-1/sa-1-patch-small.gif                        |383  |\n/://spacelink.msfc.nasa.gov                                      |381  |\n/images/crawlerway-logo.gif                                      |374  |\n/elv/DELTA/uncons.htm                                            |372  |\n/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif|359  |\n/images/nasa-logo.gif                                            |319  |\n/shuttle/resources/orbiters/atlantis.gif                         |314  |\n/history/apollo/apollo-13.html                                   |304  |\n/shuttle/resources/orbiters/discovery.gif                        |263  |\n/shuttle/missions/sts-71/images/KSC-95EC-0916.txt                |190  |\n/shuttle/resources/orbiters/challenger.gif                       |170  |\n/shuttle/missions/technology/sts-newsref/stsref-toc.html         |158  |\n/history/apollo/images/little-joe.jpg                            |150  |\n/images/lf-logo.gif                                              |143  |\n/history/apollo/publications/sp-350/sp-350.txt~                  |140  |\n+-----------------------------------------------------------------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn: Q11: Visualizing 404 Errors per Day\n\nLet's explore the 404 records temporally now. Similar to the example showing the number of unique daily hosts, break down the 404 requests by day and get the daily counts sorted by day in `errors_by_date_sorted_df`.\n\n- Display the results as a pandas dataframe \n- Also visualize the same dataframe then as a line chart"],"metadata":{"colab_type":"text","id":"UpQl8xaPXvD8","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c672b44-7460-40e1-ad72-b428443a9606"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nerrors_by_date_sorted_df = (not_found_df\n                                .select(not_found_df.host, \n                                       F.dayofmonth(\"time\").alias(\"day\"))\n                                .dropDuplicates()\n                                .groupby('day')\n                                .count()\n                                .sort('day')\n                           )\n\nerrors_by_date_sorted_df = errors_by_date_sorted_df.toPandas()\nerrors_by_date_sorted_df.T"],"metadata":{"colab_type":"code","id":"69N8tHpzXvD9","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16473e8c-dc96-4394-b453-faa868453e4f"},"colab":{},"outputId":"1746aabb-b068-43c0-bbe6-580c3fce7639"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>day</th>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n      <td>11</td>\n      <td>12</td>\n      <td>13</td>\n      <td>14</td>\n      <td>15</td>\n      <td>16</td>\n      <td>17</td>\n      <td>18</td>\n      <td>19</td>\n      <td>20</td>\n      <td>21</td>\n      <td>22</td>\n      <td>23</td>\n      <td>24</td>\n      <td>25</td>\n      <td>26</td>\n      <td>27</td>\n      <td>28</td>\n      <td>29</td>\n      <td>30</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>count</th>\n      <td>268</td>\n      <td>147</td>\n      <td>382</td>\n      <td>374</td>\n      <td>367</td>\n      <td>453</td>\n      <td>470</td>\n      <td>300</td>\n      <td>290</td>\n      <td>348</td>\n      <td>332</td>\n      <td>329</td>\n      <td>356</td>\n      <td>388</td>\n      <td>284</td>\n      <td>267</td>\n      <td>376</td>\n      <td>319</td>\n      <td>293</td>\n      <td>302</td>\n      <td>324</td>\n      <td>233</td>\n      <td>313</td>\n      <td>337</td>\n      <td>423</td>\n      <td>347</td>\n      <td>337</td>\n      <td>267</td>\n      <td>230</td>\n      <td>261</td>\n      <td>273</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[60]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>day</th>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n      <td>11</td>\n      <td>12</td>\n      <td>13</td>\n      <td>14</td>\n      <td>15</td>\n      <td>16</td>\n      <td>17</td>\n      <td>18</td>\n      <td>19</td>\n      <td>20</td>\n      <td>21</td>\n      <td>22</td>\n      <td>23</td>\n      <td>24</td>\n      <td>25</td>\n      <td>26</td>\n      <td>27</td>\n      <td>28</td>\n      <td>29</td>\n      <td>30</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>count</th>\n      <td>268</td>\n      <td>147</td>\n      <td>382</td>\n      <td>374</td>\n      <td>367</td>\n      <td>453</td>\n      <td>470</td>\n      <td>300</td>\n      <td>290</td>\n      <td>348</td>\n      <td>332</td>\n      <td>329</td>\n      <td>356</td>\n      <td>388</td>\n      <td>284</td>\n      <td>267</td>\n      <td>376</td>\n      <td>319</td>\n      <td>293</td>\n      <td>302</td>\n      <td>324</td>\n      <td>233</td>\n      <td>313</td>\n      <td>337</td>\n      <td>423</td>\n      <td>347</td>\n      <td>337</td>\n      <td>267</td>\n      <td>230</td>\n      <td>261</td>\n      <td>273</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["errors_by_date_sorted_df.plot(x='day', y='count', kind='line')"],"metadata":{"colab":{},"colab_type":"code","id":"y-pFX8dJXvD_","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0690936c-9e01-4a1f-9734-682215c0f5e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/7aaa3a7c-e8c4-486a-8487-eddf51a64c43.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}}],"execution_count":0},{"cell_type":"markdown","source":["## Your Turn: Q12: Visualizing Hourly 404 Errors\n\nUsing the DataFrame `not_found_df` you cached in the Q10, group and sort by hour of the day in increasing order, to create a DataFrame containing the total number of 404 responses for HTTP requests for each hour of the day (midnight starts at 0). \n\n- Remember to check out the [__hour__](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.hour) function and use it (we have already imported __`pyspark.sql.functions`__ as __`F`__ earlier\n- Output should be a bar graph displaying the total number of 404 errors per hour"],"metadata":{"colab_type":"text","id":"lRY2RYHeXvED","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30aa6b5d-a8b5-4307-a7ce-6adcb8ceac1a"}}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n\nhourly_avg_errors_sorted_df = (not_found_df\n                                   .select(not_found_df.host, \n                                       F.hour('time').alias('hour'))\n                                .dropDuplicates()\n                                .groupby('hour')\n                                .count()\n                                .sort('hour')\n                              ).toPandas()"],"metadata":{"colab_type":"code","id":"T7Y0Dbz3XvEF","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24973b82-e18e-450c-9e45-168b10019e4d"},"colab":{},"outputId":"873d829e-6b6d-4dc1-b894-0796ef91aec1"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["hourly_avg_errors_sorted_df.plot(x='hour', y='count', kind='bar')"],"metadata":{"colab":{},"colab_type":"code","id":"R1vsAPs1XvEJ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3d3090b-e8a1-4bdb-b512-e516d92a9a49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/e2f1a602-b1f7-4b38-b27c-b30e81a15add.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}}],"execution_count":0},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"colab_type":"text","id":"GYWAsa8mY425","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72af2882-815f-4f3c-98c5-0edf5014bba7"}}},{"cell_type":"markdown","source":["Up till now, you have completed data extraction, data transformation, and some exploratory data analysis. In the end of this project, we will complete the last step of ETL process: data loading, so the data after  your processing, wrangling, cleaning, can be used by either yourself or other colleagues later. Since we have gone through a few iteration of data processing and data wrangling, it is a good idea to make sure which one is the current dataframe you want to store and load."],"metadata":{"colab_type":"text","id":"woEUWlYCY9h0","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16875d29-cee6-4af0-897f-99ac143e57a0"}}},{"cell_type":"markdown","source":["## Your Turn: Q13: Check data integrity before loading"],"metadata":{"colab_type":"text","id":"gde50KFwZCyR","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d2002de-7505-46fb-a0de-51451d2c0c51"}}},{"cell_type":"code","source":["# TODO: Review the data frame you will like to store and load. Replace <FILL IN> with appropriate code\n\nprint(not_found_df.count())\nprint(not_found_df.columns)"],"metadata":{"colab_type":"code","id":"Jp9HtJDZZDgm","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b62c3bff-d4ea-4c32-9e7b-0061be21d5d4"},"colab":{},"outputId":"d94fda2f-0e91-401a-eb33-dfa58e5b8f0d"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">20899\n[&#39;host&#39;, &#39;method&#39;, &#39;endpoint&#39;, &#39;protocol&#39;, &#39;status&#39;, &#39;content_size&#39;, &#39;time&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">20899\n[&#39;host&#39;, &#39;method&#39;, &#39;endpoint&#39;, &#39;protocol&#39;, &#39;status&#39;, &#39;content_size&#39;, &#39;time&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["To save your dataframe in CSV file format, you call simply replace the name of the dataframe and assign file name in the following:"],"metadata":{"colab_type":"text","id":"6KT_ajIJZFsi","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94911dd5-9f40-4024-88dd-9e3d59bf6387"}}},{"cell_type":"markdown","source":["## Your Turn: Q14: Save your data as a CSV file"],"metadata":{"colab_type":"text","id":"GuiBAZEqZJT1","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24616d7d-4844-45f6-82eb-a9a28ff41178"}}},{"cell_type":"code","source":["# TODO: Review the data frame you will like to store and load. Replace <FILL IN> with appropriate code\n\nnot_found_df.write.save(\"logRecord404StatusCodeInCSV\", format = 'csv')"],"metadata":{"colab":{},"colab_type":"code","id":"DHLpKGA7ZP8P","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a8368aa-8f82-41c2-8999-fb9ccb5b3ad3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3712.save.\n: org.apache.spark.sql.AnalysisException: path dbfs:/logRecord404StatusCodeInCSV already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:157)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:173)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033921&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># TODO: Review the data frame you will like to store and load. Replace &lt;FILL IN&gt; with appropriate code</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>not_found_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;logRecord404StatusCodeInCSV&#34;</span><span class=\"ansi-blue-fg\">,</span> format <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#39;csv&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/logRecord404StatusCodeInCSV already exists.;&#39;</div>","errorSummary":"org.apache.spark.sql.AnalysisException: path dbfs:/logRecord404StatusCodeInCSV already exists.;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3712.save.\n: org.apache.spark.sql.AnalysisException: path dbfs:/logRecord404StatusCodeInCSV already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:157)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:173)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033921&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># TODO: Review the data frame you will like to store and load. Replace &lt;FILL IN&gt; with appropriate code</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>not_found_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;logRecord404StatusCodeInCSV&#34;</span><span class=\"ansi-blue-fg\">,</span> format <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#39;csv&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/logRecord404StatusCodeInCSV already exists.;&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Check to see if you have stored and loaded the CSV file successfully by checking the first 5 rows. Replace <FILL IN> with appropriate code\n\n#spark_session\\ Original line variable not defined\nspark\\\n\t.sparkContext\\\n\t.textFile(\"logRecord404StatusCodeInCSV\")\\\n\t.take(5)"],"metadata":{"colab":{},"colab_type":"code","id":"EJN6Xy76ZTkW","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54ff4975-5a90-4160-a8fa-a1fd95d797c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[68]: [&#39;js002.cc.utsunomiya-u.ac.jp,GET,/shuttle/resources/orbiters/discovery.gif,HTTP/1.0,404,&#34;&#34;,1995-08-01T00:07:33.000Z&#39;,\n &#39;tia1.eskimo.com,GET,/pub/winvn/release.txt,HTTP/1.0,404,&#34;&#34;,1995-08-01T00:28:41.000Z&#39;,\n &#39;grimnet23.idirect.com,GET,/www/software/winvn/winvn.html,HTTP/1.0,404,&#34;&#34;,1995-08-01T00:50:12.000Z&#39;,\n &#39;miriworld.its.unimelb.edu.au,GET,/history/history.htm,HTTP/1.0,404,&#34;&#34;,1995-08-01T01:04:54.000Z&#39;,\n &#39;ras38.srv.net,GET,/elv/DELTA/uncons.htm,HTTP/1.0,404,&#34;&#34;,1995-08-01T01:05:14.000Z&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[68]: [&#39;js002.cc.utsunomiya-u.ac.jp,GET,/shuttle/resources/orbiters/discovery.gif,HTTP/1.0,404,&#34;&#34;,1995-08-01T00:07:33.000Z&#39;,\n &#39;tia1.eskimo.com,GET,/pub/winvn/release.txt,HTTP/1.0,404,&#34;&#34;,1995-08-01T00:28:41.000Z&#39;,\n &#39;grimnet23.idirect.com,GET,/www/software/winvn/winvn.html,HTTP/1.0,404,&#34;&#34;,1995-08-01T00:50:12.000Z&#39;,\n &#39;miriworld.its.unimelb.edu.au,GET,/history/history.htm,HTTP/1.0,404,&#34;&#34;,1995-08-01T01:04:54.000Z&#39;,\n &#39;ras38.srv.net,GET,/elv/DELTA/uncons.htm,HTTP/1.0,404,&#34;&#34;,1995-08-01T01:05:14.000Z&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Similarly, you can also store and load your dataframe as a JSON file by completing the following:"],"metadata":{"colab_type":"text","id":"nkz9rKU3ZV7S","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a2e2852-4eb9-47df-9041-5df9f16f4468"}}},{"cell_type":"markdown","source":["## Your Turn: Q15: Save your data as a JSON file"],"metadata":{"colab_type":"text","id":"l6N5oSjgZaiX","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f76b0b-43c5-490a-91a0-a8c82153c386"}}},{"cell_type":"code","source":["# TODO: Review the data frame you will like to store and load. Replace <FILL IN> with appropriate code\n\nnot_found_df.write.save(\"logRecord404StatusCodeInJson\", format = 'json')"],"metadata":{"colab":{},"colab_type":"code","id":"nPp5cHZeZZCQ","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce218c78-3d3a-4e74-b027-acc751a25cc0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3934.save.\n: org.apache.spark.sql.AnalysisException: path dbfs:/logRecord404StatusCodeInJson already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:157)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:173)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033925&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># TODO: Review the data frame you will like to store and load. Replace &lt;FILL IN&gt; with appropriate code</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>not_found_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;logRecord404StatusCodeInJson&#34;</span><span class=\"ansi-blue-fg\">,</span> format <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#39;json&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/logRecord404StatusCodeInJson already exists.;&#39;</div>","errorSummary":"org.apache.spark.sql.AnalysisException: path dbfs:/logRecord404StatusCodeInJson already exists.;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3934.save.\n: org.apache.spark.sql.AnalysisException: path dbfs:/logRecord404StatusCodeInJson already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:157)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:173)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:711)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:307)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:235)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3632668296033925&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># TODO: Review the data frame you will like to store and load. Replace &lt;FILL IN&gt; with appropriate code</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>not_found_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;logRecord404StatusCodeInJson&#34;</span><span class=\"ansi-blue-fg\">,</span> format <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#39;json&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, path, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    737</span>             self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    738</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 739</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    740</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    741</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.4</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path dbfs:/logRecord404StatusCodeInJson already exists.;&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# TODO: Similarly, check the first 5 rows in the JSON file. Replace <FILL IN> with appropriate code\n\n# spark_session\\ Original line variable not defined\nspark\\\n\t.sparkContext\\\n\t.textFile(\"logRecord404StatusCodeInJson\")\\\n\t.take(5)"],"metadata":{"colab":{},"colab_type":"code","id":"56w3oyaOZgiE","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3553edc-1a6c-45b9-a6a3-17a71a4cdc39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[70]: [&#39;{&#34;host&#34;:&#34;js002.cc.utsunomiya-u.ac.jp&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/shuttle/resources/orbiters/discovery.gif&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T00:07:33.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;tia1.eskimo.com&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/pub/winvn/release.txt&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T00:28:41.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;grimnet23.idirect.com&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/www/software/winvn/winvn.html&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T00:50:12.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;miriworld.its.unimelb.edu.au&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/history/history.htm&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T01:04:54.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;ras38.srv.net&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/elv/DELTA/uncons.htm&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T01:05:14.000Z&#34;}&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[70]: [&#39;{&#34;host&#34;:&#34;js002.cc.utsunomiya-u.ac.jp&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/shuttle/resources/orbiters/discovery.gif&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T00:07:33.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;tia1.eskimo.com&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/pub/winvn/release.txt&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T00:28:41.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;grimnet23.idirect.com&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/www/software/winvn/winvn.html&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T00:50:12.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;miriworld.its.unimelb.edu.au&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/history/history.htm&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T01:04:54.000Z&#34;}&#39;,\n &#39;{&#34;host&#34;:&#34;ras38.srv.net&#34;,&#34;method&#34;:&#34;GET&#34;,&#34;endpoint&#34;:&#34;/elv/DELTA/uncons.htm&#34;,&#34;protocol&#34;:&#34;HTTP/1.0&#34;,&#34;status&#34;:&#34;404&#34;,&#34;content_size&#34;:&#34;&#34;,&#34;time&#34;:&#34;1995-08-01T01:05:14.000Z&#34;}&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["There is a lot more you can do about data storing and loading in terms of data formats and settings. Check out more about these options [__here__](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)."],"metadata":{"colab_type":"text","id":"P9zYYUIcZjUU","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baf09ecd-ce3d-4679-8ace-e237644d1a52"}}},{"cell_type":"markdown","source":["### Congratulations! You have finished the mini-project for this unit!"],"metadata":{"colab_type":"text","id":"lLXpioWzZlug","application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b67906e3-6b2b-46ad-855f-b87183863702"}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"Mini_Project_Data_Wrangling_at_Scale_with_Spark_Solutions_checkpoint","notebookId":3935650049725854,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"application/vnd.databricks.v1+notebook":{"notebookName":"Mini_Project_Data_Wrangling_at_Scale_with_Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3632668296033799},"colab":{"name":"Mini_Project_Data_Wrangling_at_Scale_with_Spark_checkpoint.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}
